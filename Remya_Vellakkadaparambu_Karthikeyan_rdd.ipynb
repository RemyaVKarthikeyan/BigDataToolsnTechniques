{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29678116-d78c-4902-8a0d-b1ccf9426c91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Defining reusuable codes for Clinical Trial datasets\n",
    "--------------------------------------------------\n",
    "#Instructions: Upload the clinical trial and pharma zip file. \n",
    "#Change the year in the fileroot (in cmd 2) to the uploaded clinicaltrial file name year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a91ea6-59ec-4262-9c01-17fadd39d239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Defining reusuable codes for Clinical Trial datasets \n",
    "fileroot = \"clinicaltrial_2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c673d0-d65d-41db-b6e0-d7ab7360bdae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extracting the year of clinical trial\n",
    "fileName = fileroot.split(\"_\")\n",
    "trial_Year = fileName[1]\n",
    "trial_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8c4ca7c-6a1a-460e-90ec-8ce8fe092c06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Copying the file to /tmp directory on driver node\n",
    "dbutils.fs.cp(\"/FileStore/tables/\" + fileroot + \".zip\", \"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a707bea7-34c3-491b-8e0a-4edb19cb63bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7983eda4-8b43-4c5a-8feb-9e6a8459532b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in local tmp directory\n",
    "dbutils.fs.ls(\"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0862fcba-9ec1-4762-9857-2498bd70f4a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Making 'fileroot' accessible by the command line\n",
    "import os\n",
    "os.environ['fileroot'] = fileroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc051cc-8d1f-476f-8e3d-2cc29ec02c39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Use the '-o' option to overwrite existing files without prompting\n",
    "unzip -o -d /tmp /tmp/$fileroot.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b08fc1-673d-454b-b541-4365fb2b70d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Moving the unzipped file to the DBFS directory\n",
    "dbutils.fs.mv(\"file:/tmp/\" + fileroot +\".csv\" , \"/FileStore/tables/\"+ fileroot +\".csv\", True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03114306-403d-43cb-8991-36993a4f2924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the unzipped file in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c695f695-16a9-418d-838b-cc81872225d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the DBFS directory\n",
    "dbutils.fs.ls (\"/FileStore/tables/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01752e40-c37c-4af9-9882-a66fe0d6b3d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/FileStore/tables/\"+ fileroot +\".csv\"\n",
    "print(dbutils.fs.head(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16dc5fe4-8f0d-49b7-9806-97b9701fb191",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"/FileStore/tables/\"+fileroot+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea39dd91-7bc2-4db6-887c-8644e7600fa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create RDD from CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# File path of the CSV file\n",
    "file_path = \"/FileStore/tables/\" + fileroot + \".csv\"\n",
    "\n",
    "# Preview the contents of the CSV file\n",
    "file_preview = dbutils.fs.head(file_path)\n",
    "print(\"Preview of the CSV file:\")\n",
    "print(file_preview)\n",
    "\n",
    "# Create an RDD from the CSV file\n",
    "clinicaltrial_rdd = sc.textFile(file_path)\n",
    "\n",
    "# Display the first few lines of the RDD\n",
    "print(\"First few lines of the RDD:\")\n",
    "for line in clinicaltrial_rdd.take(5):\n",
    "    print(line)\n",
    "\n",
    "# You can continue with further processing of the RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c838f9-2887-4b72-a66a-644bea78b9c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import trim, regexp_replace\n",
    "import re\n",
    "\n",
    "# Assuming sc is your SparkContext and the RDD is already created\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Function to clean the first field in a line\n",
    "def clean_first_field(line):\n",
    "    # Split the line into fields using tab as the delimiter\n",
    "    fields = line.split('\\t')\n",
    "    \n",
    "    # Clean the first field (first column) by removing leading and trailing quotation marks and trimming spaces\n",
    "    first_field = fields[0]\n",
    "    first_field = re.sub(r'^\"|\"$', '', first_field)  # Remove leading and trailing quotation marks\n",
    "    first_field = first_field.strip()  # Trim leading and trailing spaces\n",
    "    \n",
    "    # Replace the cleaned first field in the list of fields\n",
    "    fields[0] = first_field\n",
    "    \n",
    "    # Reconstruct the line by joining the fields with the tab delimiter\n",
    "    cleaned_line = '\\t'.join(fields)\n",
    "    \n",
    "    return cleaned_line\n",
    "\n",
    "# Function to clean the 14th field in a line\n",
    "def clean_14th_field(line):\n",
    "    # Split the line into fields using tab as the delimiter\n",
    "    fields = line.split('\\t')\n",
    "    \n",
    "    # Check if the line has at least 14 fields\n",
    "    if len(fields) < 14:\n",
    "        # Return the line unchanged if there aren't enough fields\n",
    "        return line\n",
    "    \n",
    "    # Clean the 14th field (14th column) by removing trailing commas and quotation marks\n",
    "    # `fields[13]` represents the 14th column since indexing starts at 0\n",
    "    fields[13] = re.sub(r'(\"|,)+$', '', fields[13])  # Remove trailing quotation marks and commas\n",
    "    fields[13] = fields[13].strip()  # Trim leading and trailing spaces\n",
    "    \n",
    "    # Reconstruct the line by joining the fields with the tab delimiter\n",
    "    cleaned_line = '\\t'.join(fields)\n",
    "    \n",
    "    return cleaned_line\n",
    "\n",
    "# Perform operations based on the value of fileroot\n",
    "def process_clinicaltrial_rdd(clinicaltrial_rdd, fileroot):\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Clean the first field in each line\n",
    "        clinicaltrial2023RDD = clinicaltrial_rdd.map(clean_first_field)\n",
    "        \n",
    "        # Clean the 14th field in each line\n",
    "        clinicaltrial2023RDD = clinicaltrial2023RDD.map(clean_14th_field)\n",
    "        \n",
    "        # Discard the header from the RDD\n",
    "        clinicaltrial2023RDD_no_header = clinicaltrial2023RDD.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "        \n",
    "        # Display the first few cleaned lines in the RDD (for demonstration purposes)\n",
    "        for line in clinicaltrial2023RDD_no_header.take(5):\n",
    "            print(line)\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # Extract the header row from the RDD\n",
    "        header = clinicaltrial_rdd.first()\n",
    "        \n",
    "        # Remove the header row from the RDD\n",
    "        clinicaltrial_rdd_no_header = clinicaltrial_rdd.filter(lambda line: line != header)\n",
    "        \n",
    "        # Display the first few lines without header (for demonstration purposes)\n",
    "        for line in clinicaltrial_rdd_no_header.take(5):\n",
    "            print(line)\n",
    "\n",
    "\n",
    "process_clinicaltrial_rdd(clinicaltrial_rdd, fileroot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ca031b-ede6-491a-b42a-9ee1cf4a8620",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 1.The number of studies in the dataset. You must ensure that you explicitly check distinct studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5347db35-df2f-4fc6-959d-e9b5ad53d5ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Assuming sc is your SparkContext and the RDDs are already created\n",
    "# sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def process_rdd(clinicaltrial_rdd, fileroot):\n",
    "    \"\"\"\n",
    "    Process the RDD based on the fileroot and perform the respective operations.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_rdd (RDD): The RDD containing clinical trial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Assuming clinicaltrial2023RDD_no_header is created by removing the header line from the RDD\n",
    "        clinicaltrial2023RDD_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "        \n",
    "        # Count the distinct number of studies based on the 'Id' column\n",
    "        distinct_studies_id = clinicaltrial2023RDD_no_header.map(lambda line: line.split('\\t')[0]).distinct().count()\n",
    "\n",
    "        # Count the distinct number of studies based on the 'Study Title' column\n",
    "        distinct_studies_title = clinicaltrial2023RDD_no_header.map(lambda line: line.split('\\t')[1]).distinct().count()\n",
    "\n",
    "        # Display the results\n",
    "        print(f\"Number of distinct studies based on 'Id': {distinct_studies_id}\")\n",
    "        print(f\"Number of distinct studies based on 'Study Title': {distinct_studies_title}\")\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # Discard the header line from the RDD\n",
    "        clinicaltrial_rdd_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "        # Count the distinct studies by extracting the first element from each line\n",
    "        count = clinicaltrial_rdd_no_header.map(lambda line: line.split(\"|\")[0]).distinct().count()\n",
    "\n",
    "        # Print the number of distinct studies found in the RDD\n",
    "        print(f\"The number of distinct studies in {fileroot} dataset is {count}.\")\n",
    "\n",
    "process_rdd(clinicaltrial_rdd, fileroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f19ef08-1f8e-4b09-a08c-8e5da02c031d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "def process_rdd(clinicaltrial_rdd, fileroot, trial_year=None):\n",
    "    \"\"\"\n",
    "    Process the RDD based on the fileroot and perform the respective operations.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_rdd (RDD): The RDD containing clinical trial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "        trial_year (int, optional): The trial year for clinicaltrial_2020 or clinicaltrial_2021. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Assuming clinicaltrial2023RDD is already the RDD to be processed\n",
    "\n",
    "        # Get the distinct Ids\n",
    "        distinct_ids_rdd = clinicaltrial_rdd.map(lambda line: line.split('\\t')[0]).distinct()\n",
    "\n",
    "        # Get the first 20 distinct Ids\n",
    "        first_20_distinct_ids = distinct_ids_rdd.take(20)\n",
    "\n",
    "        # Display the first 20 distinct Ids\n",
    "        print(\"First 20 distinct Ids:\")\n",
    "        for id in first_20_distinct_ids:\n",
    "            print(id)\n",
    "\n",
    "        # Get the distinct study titles\n",
    "        distinct_study_titles_rdd = clinicaltrial_rdd.map(lambda line: line.split('\\t')[1]).distinct()\n",
    "\n",
    "        # Get the first 20 distinct study titles\n",
    "        first_20_distinct_study_titles = distinct_study_titles_rdd.take(20)\n",
    "\n",
    "        # Display the first 20 distinct study titles\n",
    "        print(\"First 20 distinct study titles:\")\n",
    "        for title in first_20_distinct_study_titles:\n",
    "            print(title)\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # List down the first 100 distinct studies\n",
    "        distinct_studies = clinicaltrial_rdd.map(lambda line: line.split(\"|\")[0]).distinct().take(20)\n",
    "        print(f\"The ID of first 100 distinct studies in the clinical trial year {trial_year}:\")\n",
    "        for study in distinct_studies:\n",
    "            print(study)\n",
    "\n",
    "\n",
    "# For clinicaltrial_2023\n",
    "process_rdd(clinicaltrial_rdd, fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1513a754-623b-4224-926f-5ad471376df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming sc is your SparkContext and the RDDs are already created\n",
    "# sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def process_rdd(clinicaltrial_rdd, fileroot):\n",
    "    \"\"\"\n",
    "    Process the RDD based on the fileroot, perform respective operations, and plot results.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_rdd (RDD): The RDD containing clinical trial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Assuming clinicaltrial2023RDD_no_header is created by removing the header line from the RDD\n",
    "        clinicaltrial2023RDD_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "        \n",
    "        # Count the distinct number of studies based on the 'Id' column\n",
    "        distinct_studies_id = clinicaltrial2023RDD_no_header.map(lambda line: line.split('\\t')[0]).distinct().count()\n",
    "\n",
    "        # Count the distinct number of studies based on the 'Study Title' column\n",
    "        distinct_studies_title = clinicaltrial2023RDD_no_header.map(lambda line: line.split('\\t')[1]).distinct().count()\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Number of distinct studies based on 'Id': {distinct_studies_id}\")\n",
    "        print(f\"Number of distinct studies based on 'Study Title': {distinct_studies_title}\")\n",
    "\n",
    "        # Plot the data\n",
    "        data = [\n",
    "            ('Distinct Studies (Id)', distinct_studies_id),\n",
    "            ('Distinct Studies (Study Title)', distinct_studies_title)\n",
    "        ]\n",
    "\n",
    "        # Unzip the data\n",
    "        categories, counts = zip(*data)\n",
    "\n",
    "        # Create a bar plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(categories, counts, color=['blue', 'green'])\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distinct Number of Studies Based on \"Id\" and \"Study Title\"')\n",
    "\n",
    "        # Add hover-over data: annotate bars\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, yval, round(yval),\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # Remove header line\n",
    "        clinicaltrial_rdd_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "        # Count the distinct studies by extracting the first element from each line\n",
    "        count = clinicaltrial_rdd_no_header.map(lambda line: line.split(\"|\")[0]).distinct().count()\n",
    "\n",
    "        # Print the number of distinct studies found in the RDD\n",
    "        print(f\"The number of distinct studies in {fileroot} dataset is {count}.\")\n",
    "\n",
    "        # Plot the results for the count of distinct studies\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(f\"{fileroot}\", [count], color='blue')\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Distinct Studies in {fileroot} Dataset')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage of the process_rdd function\n",
    "process_rdd(clinicaltrial_rdd, fileroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b308c1-51d9-4e43-bb4a-e147e267238e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 2. You should list all the types (as contained in the Type column) of studies in the dataset along with the frequencies of each type. These should be ordered from most frequent to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e7d635-9990-41e8-becd-b16d025081a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Initialize SparkContext and SQLContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def process_rdd(clinicaltrial_rdd, fileroot):\n",
    "    \"\"\"\n",
    "    Process the RDD based on the fileroot, perform respective operations, and plot results.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_rdd (RDD): The RDD containing clinical trial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Filter out the header line from the RDD using zipWithIndex\n",
    "        clinicaltrial2023RDD_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] > 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "        # Define a function to extract the \"Type\" column\n",
    "        def extract_type(line):\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) > 10:  # Ensure there are at least 11 fields\n",
    "                return fields[10]  # Return the \"Type\" column (index 10)\n",
    "            else:\n",
    "                return None  # Return None if the line does not have enough fields\n",
    "\n",
    "        # Get the \"Type\" column while filtering out invalid lines\n",
    "        type_rdd = clinicaltrial2023RDD_no_header.map(extract_type).filter(lambda x: x is not None)\n",
    "\n",
    "        # Perform the counting and sorting\n",
    "        type_counts_rdd = type_rdd.map(lambda type_: (type_, 1)).reduceByKey(lambda a, b: a + b)\n",
    "        type_counts_ordered_rdd = type_counts_rdd.sortBy(lambda x: -x[1])\n",
    "        results = type_counts_ordered_rdd.collect()\n",
    "\n",
    "        # Display the results\n",
    "        print(\"Type frequencies ordered from most frequent to least frequent:\")\n",
    "        for type_, count in results:\n",
    "            print(f\"Type: {type_}, Count: {count}\")\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        type_names = []\n",
    "        type_counts = []\n",
    "        for type_, count in results:\n",
    "            type_names.append(type_)\n",
    "            type_counts.append(count)\n",
    "\n",
    "        # Plotting the type frequencies\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(type_names, type_counts, color='steelblue')\n",
    "        plt.xlabel('Type')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Type Frequencies Ordered from Most Frequent to Least Frequent')\n",
    "        plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "        plt.show()\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # Remove the header line from the RDD\n",
    "        clinicaltrial_rdd_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "        # Extract the type column from each row of the RDD\n",
    "        type_rdd = clinicaltrial_rdd_no_header.map(lambda row: row.split(\"|\")[5])\n",
    "\n",
    "        # Retrieve all distinct types\n",
    "        type_distinct = type_rdd.distinct().collect()\n",
    "\n",
    "        # Creating key-value pairs with type as key and value as 1\n",
    "        type_keyvaluePair = type_rdd.map(lambda types: (types, 1))\n",
    "\n",
    "        # Perform the counting and sorting\n",
    "        all_Type = type_keyvaluePair.reduceByKey(lambda a, b: a + b).takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "        # Display the obtained result\n",
    "        print(\"All types with their frequencies:\")\n",
    "        for types, count in all_Type:\n",
    "            print(types, count)\n",
    "\n",
    "        # Data preparation for Plotly plot\n",
    "        types = [types for types, _ in all_Type]\n",
    "        counts = [count for _, count in all_Type]\n",
    "\n",
    "        # Creating the horizontal bar plot\n",
    "        fig = go.Figure(go.Bar(\n",
    "            x=counts,\n",
    "            y=types,\n",
    "            orientation='h',  # Horizontal orientation\n",
    "            marker=dict(color='skyblue')  # Bar color\n",
    "        ))\n",
    "\n",
    "        # Adding hover-over tooltips\n",
    "        fig.update_traces(hovertemplate='<b>Type</b>: %{y}<br><b>Frequency</b>: %{x}')\n",
    "\n",
    "        # Setting layout\n",
    "        fig.update_layout(\n",
    "            title='Frequency of Each Type',\n",
    "            xaxis_title='Frequency',\n",
    "            yaxis_title='Types',\n",
    "            height=600,\n",
    "            width=800\n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "# Process the RDD based on the file root\n",
    "process_rdd(clinicaltrial_rdd, fileroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8ab3f2-29fc-4b2b-8d85-1eae375e4f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question 3. The top 5 conditions (from Conditions) with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb9d3eb-61ba-4ae0-baf2-7a5d15661037",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize SparkContext and SQLContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def process_conditions(clinicaltrial_rdd, fileroot):\n",
    "    \"\"\"\n",
    "    Process the \"Conditions\" column of the RDD based on the fileroot,\n",
    "    perform respective operations, and plot results.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_rdd (RDD): The RDD containing clinical trial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Filter out the header line from the RDD using zipWithIndex\n",
    "        clinicaltrial2023RDD_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] > 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "        # Define a function to extract the \"Conditions\" column and handle missing values\n",
    "        def extract_condition(line):\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) > 4:  # Ensure there are at least 5 fields\n",
    "                condition = fields[4]  # 'Conditions' column is at index 4\n",
    "                return condition if condition else None  # Return None if the condition is empty\n",
    "            else:\n",
    "                return None  # Return None if the line does not have enough fields\n",
    "\n",
    "        # Get the \"Conditions\" column from the RDD and filter out invalid lines\n",
    "        condition_rdd = clinicaltrial2023RDD_no_header.map(extract_condition).filter(lambda x: x is not None)\n",
    "\n",
    "        # Create key-value pairs where the key is the condition and the value is 1\n",
    "        condition_counts_rdd = condition_rdd.map(lambda condition: (condition, 1))\n",
    "\n",
    "        # Reduce the RDD by key (condition) to count occurrences\n",
    "        condition_frequencies_rdd = condition_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        # Sort the results from most frequent to least frequent\n",
    "        sorted_condition_frequencies_rdd = condition_frequencies_rdd.sortBy(lambda x: -x[1])\n",
    "\n",
    "        # Take the top 5 conditions\n",
    "        top_5_conditions = sorted_condition_frequencies_rdd.take(5)\n",
    "\n",
    "        # Display the top 5 conditions with their frequencies\n",
    "        print(\"Top 5 conditions and their frequencies:\")\n",
    "        for condition, count in top_5_conditions:\n",
    "            print(f\"Condition: {condition}, Count: {count}\")\n",
    "\n",
    "        # Convert top 5 conditions and their frequencies to a pandas DataFrame\n",
    "        data = pd.DataFrame(top_5_conditions, columns=['Condition', 'Count'])\n",
    "\n",
    "        # Plot a scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(data['Condition'], data['Count'], color='blue', alpha=0.7)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Condition')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Top 5 Conditions and Their Frequencies')\n",
    "\n",
    "        # Add text labels to each point in the plot\n",
    "        for i, (condition, count) in enumerate(top_5_conditions):\n",
    "            plt.text(data['Condition'][i], data['Count'][i], f'{count}', ha='right', va='bottom')\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # Discard the header line from the RDD\n",
    "        clinicaltrial_rdd_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] != 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "        # Extract the type column from each row of the RDD\n",
    "        condition_rdd = clinicaltrial_rdd_no_header.map(lambda row: row.split(\"|\")[7])\n",
    "\n",
    "        # Create key-value pairs with the condition as the key and value as 1\n",
    "        condition_frequency = condition_rdd.flatMap(lambda conditions: [(condition.strip(), 1) for condition in conditions.split(\",\")]).filter(lambda x: x[0] != \"\")\n",
    "\n",
    "        # Add up the frequencies for each condition and get the top 5 conditions\n",
    "        condition_top5 = condition_frequency.reduceByKey(lambda a, b: a + b).takeOrdered(5, key=lambda x: -x[1])\n",
    "\n",
    "        # Display the corresponding result\n",
    "        print(\"Top 5 conditions and their frequencies:\")\n",
    "        for condition, count in condition_top5:\n",
    "            print(condition, count)\n",
    "\n",
    "        # Data preparation for Plotly plot\n",
    "        conditions = [condition for condition, _ in condition_top5]\n",
    "        counts = [count for _, count in condition_top5]\n",
    "\n",
    "        # Creating the bar plot\n",
    "        fig = go.Figure(go.Bar(\n",
    "            x=conditions,\n",
    "            y=counts,\n",
    "            marker=dict(color='skyblue'),  # Bar color\n",
    "            hoverinfo='x+y',  # Show both x and y on hover\n",
    "        ))\n",
    "\n",
    "        # Setting layout\n",
    "        fig.update_layout(\n",
    "            title='Top 5 Conditions and Their Frequencies',\n",
    "            xaxis_title='Conditions',\n",
    "            yaxis_title='Frequency',\n",
    "            height=600,  # Set the height of the graph\n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "# Process the RDD based on the file root\n",
    "process_conditions(clinicaltrial_rdd, fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdad8e50-cffb-470c-907b-aacafb0ae55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import plotly.express as px\n",
    "\n",
    "# Initialize SparkContext and SQLContext\n",
    "# sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def process_conditions(clinicaltrial_rdd, fileroot):\n",
    "    \"\"\"\n",
    "    Process the 'Conditions' column of the RDD based on the fileroot,\n",
    "    perform respective operations, and plot results.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_rdd (RDD): The RDD containing clinical trial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Step 1: Define a function to split the 'Conditions' column by pipe ('|')\n",
    "        def split_conditions(line):\n",
    "            # Split the line by tab character to get the fields\n",
    "            fields = line.split('\\t')\n",
    "            # Ensure the line has at least 5 fields and get the 'Conditions' field\n",
    "            if len(fields) > 4:\n",
    "                conditions = fields[4]  # 'Conditions' column is at index 4\n",
    "                # Split the 'Conditions' field by pipe ('|') and return as a list\n",
    "                return [(field,) for field in conditions.split('|')]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        # Step 2: Use flatMap to split and explode the 'Conditions' column\n",
    "        clinicaltrial2023RDD_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] > 0).map(lambda line_index: line_index[0])\n",
    "        exploded_conditions_rdd = clinicaltrial2023RDD_no_header.flatMap(split_conditions)\n",
    "\n",
    "        \n",
    "        # Step 1: Group the exploded RDD by 'Conditions' and count occurrences\n",
    "        condition_counts_rdd = exploded_conditions_rdd.map(lambda condition: (condition, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        # Step 2: Sort the RDD by count in descending order\n",
    "        condition_counts_sorted_rdd = condition_counts_rdd.sortBy(lambda x: -x[1])\n",
    "\n",
    "        # Step 3: Take the top 5 conditions\n",
    "        top_5_conditions_rdd = condition_counts_sorted_rdd.take(5)\n",
    "\n",
    "        # Display the top 5 conditions with their frequencies\n",
    "        print(\"Top 5 conditions and their frequencies:\")\n",
    "        for condition, count in top_5_conditions_rdd:\n",
    "            print(f\"Condition: {condition[0]}, Count: {count}\")\n",
    "\n",
    "        # Convert conditions and counts to a DataFrame\n",
    "        data = {\n",
    "            'Condition': [condition[0] for condition, count in top_5_conditions_rdd],\n",
    "            'Count': [count for condition, count in top_5_conditions_rdd]\n",
    "        }\n",
    "\n",
    "        # Create a plotly bar chart with hover data and different colors for each bar\n",
    "        fig = px.bar(\n",
    "            data,\n",
    "            x='Count',\n",
    "            y='Condition',\n",
    "            orientation='h',\n",
    "            color='Condition',\n",
    "            color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "            labels={'Count': 'Frequency', 'Condition': 'Condition'},\n",
    "            title='Top 5 Conditions and Their Frequencies',\n",
    "            hover_data={'Condition': True, 'Count': True}\n",
    "        )\n",
    "\n",
    "        # Invert y-axis to display conditions from top to bottom\n",
    "        fig.update_yaxes(autorange='reversed')\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # This operation is not applicable for these file roots\n",
    "        print(f\"This operation is not applicable to {fileroot} file.\")\n",
    "\n",
    "\n",
    "# Process the RDD based on the file root\n",
    "process_conditions(clinicaltrial_rdd, fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284cbe16-1b9d-4ab7-b65a-ca1122d89fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install mplcursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a684cfbe-3e00-4825-89c5-adcb6aeb4148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "\n",
    "# Initialize SparkContext and SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"ClinicalTrialAnalysis\").getOrCreate()\n",
    "\n",
    "def process_clinicaltrial_data(file_path, fileroot):\n",
    "    \"\"\"\n",
    "    Process the clinical trial data based on the fileroot value.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path to the clinical trial data file.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the data file into an RDD\n",
    "    clinicaltrial_rdd = sc.textFile(file_path)\n",
    "\n",
    "    # Filter out the header line from the RDD using zipWithIndex\n",
    "    clinicaltrial_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] > 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Step 1: Define a function to clean and trim the 'Conditions' field\n",
    "        def clean_conditions(line):\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) > 4:  # Ensure there are at least 5 fields\n",
    "                conditions = fields[4]\n",
    "                if conditions:\n",
    "                    # Remove quotes and trim leading and trailing whitespaces\n",
    "                    conditions = conditions.replace('\"', '').strip()\n",
    "                    # Replace square brackets and parentheses\n",
    "                    conditions = re.sub(r'[\\[\\]\\(\\)]', '', conditions)\n",
    "                    # Split the conditions by pipe ('|') and flatten nested conditions\n",
    "                    conditions = re.split(r'[|,]', conditions)\n",
    "                    # Trim leading and trailing whitespaces from each condition\n",
    "                    conditions = [condition.strip() for condition in conditions]\n",
    "                    # Filter out conditions that contain 'e.g.' (case-insensitive)\n",
    "                    conditions = [condition for condition in conditions if 'e.g.' not in condition.lower()]\n",
    "                    return conditions\n",
    "            return []\n",
    "\n",
    "        # Create an RDD from the cleaned data\n",
    "        cleaned_conditions_rdd = clinicaltrial_no_header.flatMap(clean_conditions)\n",
    "\n",
    "        # Step 10: Calculate the top 5 conditions with their frequencies in RDD\n",
    "        # Map each condition to a key-value pair (condition, 1)\n",
    "        condition_counts_rdd = cleaned_conditions_rdd.map(lambda condition: (condition, 1))\n",
    "\n",
    "        # Reduce by key to count occurrences of each condition\n",
    "        condition_counts_rdd = condition_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        # Sort the RDD by count in descending order\n",
    "        sorted_condition_counts_rdd = condition_counts_rdd.sortBy(lambda x: -x[1])\n",
    "\n",
    "        # Take the top 5 conditions with the highest frequencies\n",
    "        top_5_conditions_rdd = sorted_condition_counts_rdd.take(5)\n",
    "\n",
    "        # Display the top 5 conditions with their frequencies\n",
    "        print(\"Top 5 conditions and their frequencies:\")\n",
    "        for condition, count in top_5_conditions_rdd:\n",
    "            print(f\"Condition: {condition}, Count: {count}\")\n",
    "\n",
    "        # Extract conditions and counts from the top_5_conditions_rdd list\n",
    "        conditions = [condition for condition, count in top_5_conditions_rdd]\n",
    "        counts = [count for condition, count in top_5_conditions_rdd]\n",
    "\n",
    "        # Calculate cumulative sum of counts\n",
    "        cumulative_counts = [sum(counts[:i + 1]) for i in range(len(counts))]\n",
    "\n",
    "        # Create a waterfall chart\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Initialize previous value to the initial value\n",
    "        previous_value = 0\n",
    "\n",
    "        # Iterate through conditions and their counts to plot the waterfall chart\n",
    "        for i, (condition, count) in enumerate(zip(conditions, counts)):\n",
    "            if count > 0:\n",
    "                # Plot a positive change\n",
    "                bar = ax.barh(condition, count, left=previous_value, color='steelblue', edgecolor='black')\n",
    "            else:\n",
    "                # Plot a negative change (if there are negative values)\n",
    "                bar = ax.barh(condition, count, left=previous_value, color='red', edgecolor='black')\n",
    "\n",
    "            # Update the previous value for the next iteration\n",
    "            previous_value += count\n",
    "\n",
    "            # Add hover-over tooltips\n",
    "            mplcursors.cursor(bar).connect(\n",
    "                \"add\", lambda sel: sel.annotation.set_text(f\"Count: {sel.target[1]}\")\n",
    "            )\n",
    "\n",
    "        # Plot the final value as a horizontal line\n",
    "        ax.axvline(previous_value, color='grey', linestyle='--', linewidth=1.5)\n",
    "\n",
    "        # Add labels and title\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.set_title('Top 5 Conditions and Their Frequencies (Waterfall Chart)')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or \"clinicaltrial_2021\":\n",
    "        # This operation is not applicable for these file roots\n",
    "        print(f\"This operation is not applicable to {fileroot} file.\")\n",
    "\n",
    "\n",
    "process_clinicaltrial_data(file_path, fileroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "997e0316-bf4e-4904-83ae-9ce3b4a04212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fileroot1= \"pharma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be07ffa2-5b2e-46b1-b8aa-af0940d69ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Copying the file to /tmp directory on driver node\n",
    "dbutils.fs.cp(\"/FileStore/tables/\" + fileroot1 + \".zip\", \"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cad8057-044e-4d76-8ac8-7c7d01e9ab50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52ec47c-07c1-44fd-91c2-201d0f196750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in local tmp directory\n",
    "dbutils.fs.ls(\"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8137bf-7807-4125-b8cc-b7585f6f7c43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Making 'fileroot' accessible by the command line\n",
    "import os\n",
    "os.environ['fileroot1'] = fileroot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b690fe66-872f-4e32-8d5c-03195d8656b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Unzipping the file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1002a49a-c154-449a-9725-5a0e87f3918a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "unzip -o -d /tmp /tmp/$fileroot1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6d54c0-fab6-407b-8040-93add43b4e13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Moving the unzipped file to the DBFS directory\n",
    "dbutils.fs.mv(\"file:/tmp/\" + fileroot1 +\".csv\" , \"/FileStore/tables/\" , True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db7073b-9b1a-4266-a714-ec7c7ba555a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the unzipped file in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027751a4-be3a-4af5-bc2b-4cc08b3e16ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the DBFS directory\n",
    "dbutils.fs.ls (\"/FileStore/tables/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6d37d4-04e6-4d94-9512-785aff4eb5c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Listing the contents of the csv file\n",
    "dbutils.fs.head(\"/FileStore/tables/\" + fileroot1 + \".csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4343c4f-a5c5-4ee3-bc3f-11b8a421343e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file and create an RDD\n",
    "import re\n",
    "pharma_RDD = sc.textFile(\"/FileStore/tables/\" + fileroot1 + \".csv\")\n",
    "header = pharma_RDD.first()\n",
    "pharma_RDD = pharma_RDD.filter(lambda row: row != header)\n",
    " \n",
    "# Split each row based on commas by excluding commas enclosed within double quotes\n",
    "pharma_RDD = pharma_RDD.map(lambda x: re.split(',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4594380c-eacf-4a4d-a1e5-a1d439805123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract the contents of parent company field and store it in a new variable\n",
    "pharma_Parent_RDD = pharma_RDD.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3044bec0-e878-4ba4-80a7-8d999e4d841a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pharma_companies_group = set(pharma_Parent_RDD.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa76479-babe-4336-b1ca-0cac73cd129b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Remove the double quotes\n",
    "parent_companies_group = {company.replace('\"', '') for company in pharma_companies_group}\n",
    "print(parent_companies_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278b0120-1cd3-49b6-9e13-d8dab418fc15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#converting it back to rdd using parallelize()\n",
    "pharma_Parent_RDD = sc.parallelize(parent_companies_group).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1eed9a-ae7b-453e-9859-9f9f6a319089",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 4. Find the 10 most common sponsors that are not pharmaceutical companies, along with the number of clinical trials they have sponsored. Hint: For a basic implementation, you can assume that the Parent Company column contains all possible pharmaceutical companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a672cf-5919-4575-9a9d-841cc68fe3ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "def process_clinicaltrial_data(file_path, fileroot, pharma_Parent_RDD):\n",
    "    \"\"\"\n",
    "    Process the clinical trial data based on the fileroot value.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path to the clinical trial data file.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "        pharma_Parent_RDD (RDD): An RDD of pharmaceutical parent companies for filtering.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out the header line from the RDD using zipWithIndex\n",
    "    clinicaltrial_no_header = clinicaltrial_rdd.zipWithIndex().filter(lambda line_index: line_index[1] > 0).map(lambda line_index: line_index[0])\n",
    "\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Step 1: Extract the \"Sponsor\" column from the RDD\n",
    "        sponsor2023RDD = clinicaltrial_no_header.map(lambda line: line.split('\\t')[6])\n",
    "\n",
    "        # Filter out all the sponsors that are not pharmaceutical companies\n",
    "        sponsor_non_pharma_RDD = sponsor2023RDD.filter(lambda x: x not in pharma_Parent_RDD)\n",
    "\n",
    "        # Count the number of clinical trials sponsored by each non-pharmaceutical company\n",
    "        sponsor_non_pharma_count_RDD = sponsor_non_pharma_RDD.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "        # Get the top 10 non-pharma sponsors by number of clinical trials sponsored\n",
    "        sponsor_non_pharma_top10_RDD = sponsor_non_pharma_count_RDD.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "        # Display the corresponding results with header\n",
    "        print(\"Top 10 Non-Pharma Sponsors by Number of Clinical Trials Sponsored (2023):\")\n",
    "        print(\"+---------------------------------------+-----+\")\n",
    "        print(\"|{:40s}|{:5s}|\".format(\"Sponsor\", \"Count\"))\n",
    "        print(\"+---------------------------------------+-----+\")\n",
    "        for sponsor, count in sponsor_non_pharma_top10_RDD:\n",
    "            print(\"|{:40s}|{:5d}|\".format(sponsor, count))\n",
    "        print(\"+---------------------------------------+-----+\")\n",
    "\n",
    "        # Plotting a heatmap with the top 10 non-pharma sponsors and the number of clinical trials they sponsored\n",
    "        # Extract sponsors and counts from the list of tuples\n",
    "        sponsors = [item[0] for item in sponsor_non_pharma_top10_RDD]\n",
    "        counts = [item[1] for item in sponsor_non_pharma_top10_RDD]\n",
    "\n",
    "        # Convert the counts list to a 2D array (reshape it as a row with all counts)\n",
    "        counts_array = np.array(counts).reshape(1, -1)\n",
    "\n",
    "        # Create a heatmap\n",
    "        plt.figure(figsize=(12, 3))  # Adjust the size as per your preference\n",
    "        sns.heatmap(counts_array, annot=True, cmap='coolwarm', fmt='d', xticklabels=sponsors, yticklabels=['Counts'])\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.title('Top 10 Non-Pharma Sponsors by Number of Clinical Trials Sponsored (2023)')\n",
    "        plt.xlabel('Sponsor')\n",
    "        plt.ylabel('')\n",
    "\n",
    "        # Display the heatmap\n",
    "        plt.show()\n",
    "\n",
    "    elif fileroot == \"clinicaltrial_2020\" or fileroot == \"clinicaltrial_2021\":\n",
    "        # Step 1: Extract the \"Sponsor\" column from the RDD\n",
    "        sponsor_RDD = clinicaltrial_no_header.map(lambda line: line.split('|')[1])\n",
    "\n",
    "        # Filter out all the sponsors that are not pharmaceutical companies\n",
    "        sponsor_non_pharma_RDD = sponsor_RDD.filter(lambda x: x not in pharma_Parent_RDD)\n",
    "\n",
    "        # Count the number of clinical trials sponsored by each non-pharmaceutical company\n",
    "        sponsor_non_pharma_count_RDD = sponsor_non_pharma_RDD.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "        # Get the top 10 non-pharma sponsors by number of clinical trials sponsored\n",
    "        sponsor_non_pharma_top10_RDD = sponsor_non_pharma_count_RDD.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "        # Display the corresponding results with header\n",
    "        trial_Year2 = fileroot.split(\"_\")[1]  # Extract the year from the fileroot string\n",
    "        print(f\"{trial_Year2}:\")\n",
    "        print(\"+---------------------------------------+-----+\")\n",
    "        print(\"|{:40s}|{:5s}|\".format(\"Sponsor\", \"Count\"))\n",
    "        print(\"+---------------------------------------+-----+\")\n",
    "        for sponsor, count in sponsor_non_pharma_top10_RDD:\n",
    "            print(\"|{:40s}|{:5d}|\".format(sponsor, count))\n",
    "        print(\"+---------------------------------------+-----+\")\n",
    "\n",
    "# Process the data\n",
    "process_clinicaltrial_data(file_path, fileroot, pharma_Parent_RDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c8d859-4f7b-4554-b22a-00488955e4d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 5.Plot number of completed studies for each month in 2023. You need to include your visualization as well as a table of all the values you have plotted for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e850d94b-2daa-498b-826b-637dc054ebab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "\n",
    "# Function to process and plot data for clinicaltrial_2023\n",
    "def process_clinicaltrial_2023(clinicaltrial_rdd):\n",
    "    # Split each line of the RDD\n",
    "    split_rdd = clinicaltrial_rdd.map(lambda line: line.split('\\t'))\n",
    "\n",
    "    # Filter RDD to include only completed studies in 2023\n",
    "    complete_2023RDD = split_rdd.filter(lambda row: len(row) > 13 and row[3] == 'COMPLETED' and row[13].startswith('2023'))\n",
    "\n",
    "    # Initialize a defaultdict with a default value of 0\n",
    "    completed_month_status = defaultdict(int)\n",
    "\n",
    "    # Updating the count for each month\n",
    "    for row in complete_2023RDD.collect():\n",
    "        completion_date_status = row[13]  # Accessing the 13th column\n",
    "\n",
    "        # Remove all special characters from completion_date_status except '-'\n",
    "        clean_completion_date_status = re.sub(r\"[^0-9-]\", \"\", completion_date_status)\n",
    "\n",
    "        # Extract the month from the clean completion date\n",
    "        completion_month = clean_completion_date_status.split('-')[1]  # Extracting the month\n",
    "\n",
    "        # Increment the count for the month\n",
    "        completed_month_status[completion_month] += 1\n",
    "\n",
    "    # Sort the completed_month_status dictionary by month\n",
    "    sorted_completed_month_status = sorted(completed_month_status.items(), key=lambda x: int(x[0]))\n",
    "\n",
    "    # Print results in a tabular format in ascending order of months\n",
    "    print(\"+---------------+--------------------------------\")\n",
    "    print(\"| Month        | Number of completed studies of the month  |\")\n",
    "    print(\"+---------------+--------------------------------\")\n",
    "    for month, count in sorted_completed_month_status:\n",
    "        print(f\"|  {month:<10}  | {count:<25}  |\")\n",
    "    print(\"+---------------+--------------------------------\")\n",
    "\n",
    "    # Extract months and counts from the sorted dictionary\n",
    "    months = [item[0] for item in sorted_completed_month_status]\n",
    "    counts = [item[1] for item in sorted_completed_month_status]\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Use a colormap to change hues based on count\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    colors = cmap([count / max(counts) for count in counts])\n",
    "\n",
    "    # Plot bar chart\n",
    "    bars = plt.bar(months, counts, color=colors)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Completed Studies')\n",
    "    plt.title('Number of Completed Studies per Month in 2023')\n",
    "\n",
    "    # Add a color bar to represent the hue scale\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(counts), vmax=max(counts)))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, label='Count')\n",
    "\n",
    "    # Use mplcursors to display hover-over data\n",
    "    cursor = mplcursors.cursor(bars, hover=True)\n",
    "    cursor.connect(\n",
    "        \"add\", lambda sel: sel.annotation.set_text(\n",
    "            f\"Month: {months[sel.index]}\\nCount: {counts[sel.index]}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Create a line graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the line graph\n",
    "    line, = plt.plot(months, counts, color='blue', linewidth=2, marker='o', markersize=8, linestyle='-', label='Completed Studies')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Completed Studies')\n",
    "    plt.title('Number of Completed Studies per Month in 2023')\n",
    "    plt.legend()\n",
    "\n",
    "    # Use mplcursors to display hover-over data\n",
    "    cursor = mplcursors.cursor([line], hover=True)\n",
    "    cursor.connect(\n",
    "        \"add\", lambda sel: sel.annotation.set_text(\n",
    "            f\"Month: {months[sel.index]}\\nCount: {counts[sel.index]}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Function to process and plot data for clinicaltrial_2020 or clinicaltrial_2021\n",
    "def process_clinicaltrial_2020_2021(clinicaltrial_rdd, trial_year):\n",
    "    # Filter out the rows where the study status is \"Completed\" and the study completion date is in the trial year\n",
    "    status_filter_RDD = clinicaltrial_rdd.filter(lambda x: \"Completed\" in x.split(\"|\")[2] and trial_year in x.split(\"|\")[4])\n",
    "    \n",
    "    # Map each row to a tuple containing the month and year of the study completion date\n",
    "    def parse_date(row):\n",
    "        date_str = row.split(\"|\")[4]\n",
    "        # Parse the date\n",
    "        date_obj = datetime.strptime(date_str, \"%b %Y\")\n",
    "        # Return tuple with month and year\n",
    "        return date_obj.strftime(\"%b\"), date_obj.strftime(\"%Y\")\n",
    "    \n",
    "    MM_YYYY_RDD = status_filter_RDD.map(parse_date)\n",
    "    \n",
    "    # Count the number of studies completed for each month and year\n",
    "    month_wise_count_RDD = MM_YYYY_RDD.countByValue()\n",
    "    \n",
    "    # Sorting the counts by month\n",
    "    month_wise_count_sort = sorted(month_wise_count_RDD.items(), key=lambda x: datetime.strptime(x[0][0], \"%b\").month)\n",
    "    \n",
    "    # Extract the counts for each month and the months list\n",
    "    months = [month for (month, year), count in month_wise_count_sort if year == trial_year]\n",
    "    num_counts = [count for (month, year), count in month_wise_count_sort if year == trial_year]\n",
    "    \n",
    "    # Print months and counts in two columns\n",
    "    print(\"Month   Count\")\n",
    "    for month, count in zip(months, num_counts):\n",
    "        print(f\"{month:<6} {count:<6}\")\n",
    "\n",
    "    # Plotting the data as a line plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(months, num_counts, marker='o', linestyle='-')\n",
    "    plt.title(f'Completed Studies Month-wise (Year: {trial_year})')\n",
    "    plt.xlabel(f'Months (Year: {trial_year})')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the data as a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(months, num_counts, color='blue')\n",
    "    plt.title(f'Completed Studies Month-wise (Year: {trial_year})')\n",
    "    plt.xlabel(f'Months (Year: {trial_year})')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main code to process files based on the fileroot value\n",
    "def process_files(fileroot, clinicaltrial_rdd):\n",
    "    # Split the filename to get the year\n",
    "    fileName = fileroot.split(\"_\")\n",
    "    trial_Year = fileName[1]\n",
    "    \n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Process and plot data for clinicaltrial_2023\n",
    "        process_clinicaltrial_2023(clinicaltrial_rdd)\n",
    "    elif fileroot == \"clinicaltrial_2020\" or fileroot == \"clinicaltrial_2021\":\n",
    "        # Process and plot data for clinicaltrial_2020 or clinicaltrial_2021\n",
    "        process_clinicaltrial_2020_2021(clinicaltrial_rdd, trial_Year)\n",
    "        \n",
    "\n",
    "process_files(fileroot, clinicaltrial_rdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ce7715-8f45-47fd-a22b-2b5af460af4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Additional Analyses on Clinicaltrial datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3aa915-9c76-4d54-8189-c425c9ed8036",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1. Top 5 studies with the highest enrollments (2023)  and  Distribution of study statuses (Historical datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53111ea8-4247-4980-8f4f-044780e6d12a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to process and visualize data for clinicaltrial_2023\n",
    "def process_clinicaltrial_2023(clinicaltrial_rdd):\n",
    "    # Split each line of the RDD\n",
    "    split_rdd = clinicaltrial_rdd.map(lambda line: line.split('\\t'))\n",
    "\n",
    "    # Function to parse the Enrollment column and handle non-numeric and null values\n",
    "    def parse_enrollment(row):\n",
    "        try:\n",
    "            # Check if the enrollment value exists\n",
    "            if len(row) < 9 or not row[8]:\n",
    "                # Return None for missing or null values in the Enrollment column\n",
    "                return None\n",
    "\n",
    "            enrollment = row[8]\n",
    "\n",
    "            # Convert enrollment to a float\n",
    "            enrollment_float = float(enrollment)\n",
    "\n",
    "            # Return a tuple with the study ID, study title, enrollment (string), and enrollment (float)\n",
    "            return (row[0], row[1], row[8], enrollment_float)\n",
    "\n",
    "        except ValueError:\n",
    "            # Return None for non-numeric values or parsing errors\n",
    "            return None\n",
    "\n",
    "    # Parse the Enrollment column and filter out None values\n",
    "    enrollment_rdd = split_rdd.map(parse_enrollment).filter(lambda x: x is not None)\n",
    "\n",
    "    # Sort the RDD based on enrollment in descending order\n",
    "    sorted_enrollment_rdd = enrollment_rdd.sortBy(lambda x: x[3], ascending=False)\n",
    "\n",
    "    # Take the top 5 studies with the highest enrollments\n",
    "    top_5_enrollments = sorted_enrollment_rdd.take(5)\n",
    "\n",
    "    # Display the top 5 studies with the highest enrollments\n",
    "    print(\"Top 5 studies with highest enrollments:\")\n",
    "    for study in top_5_enrollments:\n",
    "        print(f\"Study ID: {study[0]}, Study Title: {study[1]}, Enrollment: {study[2]}\")\n",
    "\n",
    "    # Extracting data from the top 5 studies with the highest enrollments\n",
    "    study_titles = [study[1] for study in top_5_enrollments]\n",
    "    enrollments = [study[3] for study in top_5_enrollments]\n",
    "\n",
    "    # Calculate the total enrollment of the top 5 studies\n",
    "    total_enrollment = sum(enrollments)\n",
    "\n",
    "    # Create a pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plotting the pie chart using enrollments as sizes and study titles as labels\n",
    "    plt.pie(enrollments, labels=study_titles, autopct=lambda p: f'{p:.2f}%\\n({p * total_enrollment / 100:.0f})', startangle=140)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title('Top 5 Studies with Highest Enrollments')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Function to process and visualize data for clinicaltrial_2020 or clinicaltrial_2021\n",
    "def process_clinicaltrial_2020_2021(clinicaltrial_rdd, trial_year):\n",
    "    # Filter RDD for the given year\n",
    "    filtered_RDD = clinicaltrial_rdd.filter(lambda line: trial_year in line.split(\"|\")[4])\n",
    "\n",
    "    # Problem Statement 4: Distribution of study statuses\n",
    "    status_distribution = filtered_RDD.map(lambda line: (line.split(\"|\")[2], 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "    print(f\"Distribution of study statuses for year {trial_year}:\")\n",
    "    print(status_distribution.collect())\n",
    "\n",
    "    # Extracting status labels and counts\n",
    "    labels = status_distribution.map(lambda x: x[0]).collect()\n",
    "    counts = status_distribution.map(lambda x: x[1]).collect()\n",
    "\n",
    "    # Plotting the pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f'\\nDistribution of Study Statuses (Year: {trial_year})\\n', pad=20)  # Add padding to the title\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the bar chart\n",
    "    plt.figure(figsize=(10, 6))  # Set figure size\n",
    "    plt.barh(labels, counts, color=['blue', 'green', 'red', 'gold', 'coral', 'lightskyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral'])\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Study Status')\n",
    "    plt.title(f'Distribution of Study Statuses (Year: {trial_year})')\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlapping elements\n",
    "    plt.show()\n",
    "\n",
    "# Main code to process files based on the fileroot value\n",
    "def process_files(fileroot, clinicaltrial_rdd):\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Process and visualize data for clinicaltrial_2023\n",
    "        process_clinicaltrial_2023(clinicaltrial_rdd)\n",
    "    elif fileroot == \"clinicaltrial_2020\" or fileroot == \"clinicaltrial_2021\":\n",
    "        # Determine the trial year based on fileroot\n",
    "        trial_year = fileroot.split('_')[1]\n",
    "        # Process and visualize data for clinicaltrial_2020 or clinicaltrial_2021\n",
    "        process_clinicaltrial_2020_2021(clinicaltrial_rdd, trial_year)\n",
    "\n",
    "\n",
    "process_files(fileroot, clinicaltrial_rdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed70344e-e757-4733-ae50-c677d63c4c79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2. Comparing the sponsorship of Mayo Clinic and Massachusetts General Hospital (2023) and Top 5 sponsors with the most completed studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcb8ef2-6045-485a-b4cf-5ff8d1a95bfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to process and visualize data for clinicaltrial_2023\n",
    "def process_clinicaltrial_2023(clinicaltrial_rdd):\n",
    "    # Split each line of the RDD\n",
    "    split_rdd = clinicaltrial_rdd.map(lambda line: line.split('\\t'))\n",
    "\n",
    "    # Filter the RDD to include only studies sponsored by \"Mayo Clinic\"\n",
    "    Mayo_Clinic_studies = split_rdd.filter(lambda row: row[6].strip() == \"Mayo Clinic\")\n",
    "\n",
    "    # Count the number of studies that satisfy the condition\n",
    "    num_Mayo_Clinic_studies = Mayo_Clinic_studies.count()\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of studies sponsored by Mayo Clinic: {num_Mayo_Clinic_studies}\")\n",
    "\n",
    "    # Filter the RDD to include only studies sponsored by \"Massachusetts General Hospital\"\n",
    "    M_G_studies = split_rdd.filter(lambda row: row[6].strip() == \"Massachusetts General Hospital\")\n",
    "\n",
    "    # Count the number of studies that satisfy the condition\n",
    "    num_M_G_studies = M_G_studies.count()\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of studies sponsored by Massachusetts General Hospital: {num_M_G_studies}\")\n",
    "\n",
    "    # Define the data\n",
    "    data = {\n",
    "        'Institution': ['Mayo Clinic', 'Massachusetts General Hospital'],\n",
    "        'Number of Studies': [num_Mayo_Clinic_studies, num_M_G_studies]\n",
    "    }\n",
    "\n",
    "    # Create a pandas DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Set the index to the Institution column\n",
    "    df.set_index('Institution', inplace=True)\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df, annot=True, fmt='d', cmap='coolwarm', cbar=True, linewidths=.5, linecolor='black')\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title('Heatmap of Number of Studies Sponsored by Each Institution')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Institution')\n",
    "\n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to process and visualize data for clinicaltrial_2020 or clinicaltrial_2021\n",
    "def process_clinicaltrial_2020_2021(clinicaltrial_rdd, trial_year):\n",
    "    # Filter the RDD for studies with a \"Completed\" status\n",
    "    completed_studies = clinicaltrial_rdd.filter(lambda line: \"Completed\" in line)\n",
    "\n",
    "    # Calculate the top 5 sponsors with the most completed studies\n",
    "    top_sponsors_completed = completed_studies.map(lambda line: (line.split(\"|\")[1], 1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], ascending=False).take(5)\n",
    "\n",
    "    # Print the top 5 sponsors with the most completed studies\n",
    "    print(f\"Top 5 sponsors with the most completed studies (Year: {trial_year}):\")\n",
    "    print(top_sponsors_completed)\n",
    "\n",
    "    # Extract sponsor names and their respective counts\n",
    "    sponsors = [item[0] for item in top_sponsors_completed]\n",
    "    counts = [item[1] for item in top_sponsors_completed]\n",
    "\n",
    "    # Plotting the bar chart\n",
    "    plt.figure(figsize=(10, 6))  # Set figure size\n",
    "    plt.bar(sponsors, counts, color='skyblue')\n",
    "    plt.xlabel('Sponsor')\n",
    "    plt.ylabel('Number of Completed Studies')\n",
    "    plt.title(f'Top 5 Sponsors with the Most Completed Studies (Year: {trial_year})')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlapping elements\n",
    "    plt.show()\n",
    "\n",
    "# Define the main function to process the clinical trial data based on fileroot\n",
    "def process_files(fileroot, clinicaltrial_rdd):\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Process and visualize data for clinicaltrial_2023\n",
    "        process_clinicaltrial_2023(clinicaltrial_rdd)\n",
    "    elif fileroot == \"clinicaltrial_2020\" or fileroot == \"clinicaltrial_2021\":\n",
    "        # Determine the trial year based on fileroot\n",
    "        trial_year = fileroot.split('_')[1]\n",
    "        # Process and visualize data for clinicaltrial_2020 or clinicaltrial_2021\n",
    "        process_clinicaltrial_2020_2021(clinicaltrial_rdd, trial_year)\n",
    "\n",
    "\n",
    "process_files(fileroot, clinicaltrial_rdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fb4529-94de-4a7f-b1cc-b8b219801ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3.  Count of studies where the condition contains the keywords \"heart,\" \"alcohol,\" or \"liver,\" (2023) and list studies started in a specific year (historical datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1977c6aa-3905-4c7d-bbe7-76696de0753e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def count_studies_with_keywords(clinicaltrial_rdd):\n",
    "    # Define keywords to search for\n",
    "    keywords = {\n",
    "        \"heart\": \"heart\",\n",
    "        \"alcohol\": \"alcohol\",\n",
    "        \"liver\": \"liver\"\n",
    "    }\n",
    "\n",
    "    # Split each line of the RDD based on tab delimiter (adjust if your dataset uses a different delimiter)\n",
    "    split_rdd = clinicaltrial_rdd.map(lambda line: line.split('\\t'))\n",
    "\n",
    "    # Initialize a dictionary to hold the counts for each keyword\n",
    "    keyword_counts = {}\n",
    "    \n",
    "    # Calculate the counts for each keyword\n",
    "    for keyword_name, keyword in keywords.items():\n",
    "        # Filter the RDD to include only rows where the condition contains the keyword\n",
    "        filtered_rdd = split_rdd.filter(lambda row: keyword in row[4].lower())\n",
    "        \n",
    "        # Count the number of studies in the filtered RDD\n",
    "        keyword_counts[keyword_name] = filtered_rdd.count()\n",
    "\n",
    "    # Print the count of studies where the condition contains each of the keywords\n",
    "    for keyword_name, count in keyword_counts.items():\n",
    "        print(f\"Count of studies where the condition contains '{keyword_name}': {count}\")\n",
    "\n",
    "    # Plot the counts as a bar chart\n",
    "    plot_study_keyword_counts(keyword_counts)\n",
    "\n",
    "\n",
    "def plot_study_keyword_counts(keyword_counts):\n",
    "    # Extract conditions and counts from the dictionary\n",
    "    conditions = list(keyword_counts.keys())\n",
    "    counts = list(keyword_counts.values())\n",
    "\n",
    "    # Colors for the bar chart\n",
    "    colors = ['red', 'orange', 'green']\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(conditions, counts, color=colors)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Condition')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Number of Studies with Conditions Containing \"Heart,\" \"Alcohol,\" or \"Liver\"')\n",
    "\n",
    "    # Add hover-over data: annotate bars\n",
    "    for bar in bars:\n",
    "        # Get the height of the bar (count value)\n",
    "        yval = bar.get_height()\n",
    "        # Add text annotation above the bar to display the count value\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{int(yval)}',\n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define a function to list studies started in a particular year\n",
    "def list_studies_by_year(year, clinicaltrial_rdd):\n",
    "    # Filter RDD to get studies started in the specified year\n",
    "    studies_year = clinicaltrial_rdd.filter(lambda line: year in line.split(\"|\")[3])\n",
    "\n",
    "    # Map each study to a tuple containing study ID, sponsor, and start date\n",
    "    studies_year_info = studies_year.map(lambda line: (line.split(\"|\")[0], line.split(\"|\")[1], line.split(\"|\")[3]))\n",
    "\n",
    "    # Print the header\n",
    "    print(\"Study ID\\t\\t Sponsor\\t\\t Start Date\")\n",
    "    print(\"-\" * 40)  # Print a separator line\n",
    "\n",
    "    # Print the study information\n",
    "    for study_id, sponsor, start_date in studies_year_info.collect():\n",
    "        print(f\"{study_id}\\t {sponsor}\\t {start_date}\")\n",
    "\n",
    "\n",
    "# Main function to process the clinical trial data based on fileroot\n",
    "def process_files(fileroot, clinicaltrial_rdd):\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Perform tasks for clinicaltrial_2023\n",
    "        count_studies_with_keywords(clinicaltrial_rdd)\n",
    "    elif fileroot == \"clinicaltrial_2020\" or fileroot == \"clinicaltrial_2021\":\n",
    "        # Specify the year from the fileroot (e.g., '2020' or '2021')\n",
    "        trial_year = fileroot.split('_')[1]\n",
    "        # Perform tasks for clinicaltrial_2020 or clinicaltrial_2021\n",
    "        list_studies_by_year(trial_year, clinicaltrial_rdd)\n",
    "\n",
    "\n",
    "process_files(fileroot, clinicaltrial_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835f31ea-3a9c-4e08-8f51-a05023bbf088",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing the files from local directory if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a8fee7-e1b7-4239-bb50-b7fbcaa6d993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/FileStore/tables/\" + fileroot + \".zip\")\n",
    "dbutils.fs.rm(\"/FileStore/tables/\" + fileroot1 + \".zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae936e7-96f8-4fc9-92ff-da783c4ffed5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory_path = \"/FileStore/tables/\"\n",
    "\n",
    "# Use dbutils.fs.rm to delete files in the directory\n",
    "# The recursive parameter is set to True to delete all files in the directory\n",
    "dbutils.fs.rm(directory_path, recurse=True)\n",
    "\n",
    "# Print a message indicating the deletion\n",
    "print(f\"All files in {directory_path} have been deleted.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4031844513450311,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Remya_Vellakkadaparambu_Karthikeyan_rdd",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
