{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5889174e-8314-4bf0-bfc5-eb9fd81020a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Defining reusuable codes for Clinical Trial datasets\n",
    "--------------------------------------------------\n",
    "#Instructions: Upload the clinical trial and pharma zip file. \n",
    "#Change the year in the fileroot (in cmd 2) to the uploaded clinicaltrial file name year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f935607d-7859-44c8-8465-ec5b843d00ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Defining reusuable codes for Clinical Trial datasets\n",
    "fileroot = \"clinicaltrial_2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623a6bae-e7d4-4c43-bd3b-c9005b3dbdca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extracting the year of clinical trial\n",
    "fileName = fileroot.split(\"_\")\n",
    "trial_Year = fileName[1]\n",
    "trial_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50387c2-d5c4-4bcc-b772-b773007abc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Copying the file to /tmp directory on driver node\n",
    "dbutils.fs.cp(\"/FileStore/tables/\" + fileroot + \".zip\", \"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ee5db2-be88-4f50-a53a-3e3853bb1b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf40931-3b6b-4074-92f3-f99e7f0a7b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in local tmp directory\n",
    "dbutils.fs.ls(\"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3c4e84-8506-4cfb-979d-bc55d5abde99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Making 'fileroot' accessible by the command line\n",
    "import os\n",
    "os.environ['fileroot'] = fileroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11a43cb-0ceb-4105-8277-76c2a6dc3b37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Use the '-o' option to overwrite existing files without prompting\n",
    "unzip -o -d /tmp /tmp/$fileroot.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa86a091-0e8b-44e8-8c51-38a9a5b72089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Moving the unzipped file to the DBFS directory\n",
    "dbutils.fs.mv(\"file:/tmp/\" + fileroot +\".csv\" , \"/FileStore/tables/\"+ fileroot +\".csv\", True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766a178d-a209-4a0b-8a0f-90f8e39cd87e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the unzipped file in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5c3e32-65fb-4e24-a8c1-aaed45192ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the DBFS directory\n",
    "dbutils.fs.ls (\"/FileStore/tables/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be64b1f9-ac18-42d3-8312-a283d82e81b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/FileStore/tables/\"+ fileroot +\".csv\"\n",
    "print(dbutils.fs.head(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdfb39c-c645-4814-956a-f68472694dbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"/FileStore/tables/\"+fileroot+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf502b3-8334-4586-a9a6-26e742c0d7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "# Create or get a Spark session (modify appName as needed)\n",
    "spark = SparkSession.builder.appName(\"ClinicalTrialData\").getOrCreate()\n",
    "\n",
    "# Define the schema for clinicaltrial_2023\n",
    "schema_2023 = StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"Study Title\", StringType(), True),\n",
    "    StructField(\"Acronym\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Conditions\", StringType(), True),\n",
    "    StructField(\"Interventions\", StringType(), True),\n",
    "    StructField(\"Sponsor\", StringType(), True),\n",
    "    StructField(\"Collaborators\", StringType(), True),\n",
    "    StructField(\"Enrollment\", StringType(), True),\n",
    "    StructField(\"Funder Type\", StringType(), True),\n",
    "    StructField(\"Type\", StringType(), True),\n",
    "    StructField(\"Study Design\", StringType(), True),\n",
    "    StructField(\"Start\", StringType(), True),\n",
    "    StructField(\"Completion\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for clinicaltrial_2020\n",
    "schema_2020 = StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"Sponsor\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Start\", StringType(), True),\n",
    "    StructField(\"Completion\", StringType(), True),\n",
    "    StructField(\"Type\", StringType(), True),\n",
    "    StructField(\"Submission\", DateType(), True),\n",
    "    StructField(\"Conditions\", StringType(), True),\n",
    "    StructField(\"Interventions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/FileStore/tables/\" + fileroot + \".csv\"\n",
    "\n",
    "# Determine the file type and apply the appropriate schema and delimiter\n",
    "if \"clinicaltrial_2023\" in fileroot:\n",
    "    # Read the CSV file using the schema for clinicaltrial_2023\n",
    "    clinicaltrial_df = spark.read.format(\"csv\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option(\"quote\",\"\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .schema(schema_2023)\\\n",
    "        .load(file_path)\n",
    "        \n",
    "elif \"clinicaltrial_2020\" in fileroot or \"clinicaltrial_2021\" in fileroot:\n",
    "    # Read the CSV file using the schema for clinicaltrial_2020\n",
    "    clinicaltrial_df = spark.read.format(\"csv\")\\\n",
    "        .option(\"delimiter\", \"|\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .schema(schema_2020)\\\n",
    "        .load(file_path)\n",
    "else:\n",
    "    print(\"Unknown file type in fileroot\")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(clinicaltrial_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb769ce-e7f4-4ec2-99f3-b8c512734a64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, regexp_replace, col\n",
    "\n",
    "# Perform cleaning operations only if the fileroot is clinicaltrial_2023\n",
    "if \"clinicaltrial_2023\" in fileroot:\n",
    "    # Clean the first column of the DataFrame\n",
    "    clinicaltrial_df = clinicaltrial_df.withColumn(\n",
    "        clinicaltrial_df.columns[0],  # First column\n",
    "        regexp_replace(col(clinicaltrial_df.columns[0]), r'^\"|\"$', '')  # Remove leading and trailing quotation marks\n",
    "    ).withColumn(\n",
    "        clinicaltrial_df.columns[0],  # First column\n",
    "        trim(col(clinicaltrial_df.columns[0]))  # Trim leading and trailing spaces\n",
    "    )\n",
    "\n",
    "    # Clean the 14th column of the DataFrame\n",
    "    clinicaltrial_df = clinicaltrial_df.withColumn(\n",
    "        clinicaltrial_df.columns[13],  # 14th column (index 13)\n",
    "        regexp_replace(col(clinicaltrial_df.columns[13]), r'(\"|,)+$', '')  # Remove trailing quotation marks and commas\n",
    "    ).withColumn(\n",
    "        clinicaltrial_df.columns[13],  # 14th column (index 13)\n",
    "        trim(col(clinicaltrial_df.columns[13]))  # Trim leading and trailing spaces\n",
    "    )\n",
    "\n",
    "    # Display the cleaned DataFrame\n",
    "    display(clinicaltrial_df)\n",
    "else:\n",
    "    # If not clinicaltrial_2023, display the DataFrame as is\n",
    "    display(clinicaltrial_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f578d7f-bd65-4bc0-8e84-4b3ba9340b5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 1. The number of studies in the dataset. You must ensure that you explicitly check distinct studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaaacf4e-0830-493c-91ed-8cdae3e7fdf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_distinct_studies(df, id_col, title_col):\n",
    "    \"\"\"\n",
    "    Calculate the distinct counts of studies based on 'Id' and 'Study Title' columns.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing clinical trial data.\n",
    "        id_col (str): The column name for 'Id'.\n",
    "        title_col (str): The column name for 'Study Title'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the distinct counts for 'Id' and 'Study Title'.\n",
    "    \"\"\"\n",
    "    # Count the distinct number of studies based on the 'Id' column\n",
    "    distinct_studies_id = df.select(id_col).distinct().count()\n",
    "\n",
    "    # Count the distinct number of studies based on the 'Study Title' column\n",
    "    distinct_studies_title = df.select(title_col).distinct().count()\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Number of distinct studies based on '{id_col}': {distinct_studies_id}\")\n",
    "    print(f\"Number of distinct studies based on '{title_col}': {distinct_studies_title}\")\n",
    "\n",
    "    # Return a dictionary with the results\n",
    "    return {\n",
    "        \"distinct_studies_id\": distinct_studies_id,\n",
    "        \"distinct_studies_title\": distinct_studies_title\n",
    "    }\n",
    "\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with\n",
    "# Define the column names based on the DataFrame's structure\n",
    "# For clinicaltrial_2023, the column names are 'Id' and 'Study Title'\n",
    "# For clinicaltrial_2020 and clinicaltrial_2021, the column names may be different\n",
    "\n",
    "if \"clinicaltrial_2023\" in fileroot:\n",
    "    # Define column names for clinicaltrial_2023\n",
    "    id_col = \"Id\"\n",
    "    title_col = \"Study Title\"\n",
    "elif \"clinicaltrial_2020\" in fileroot or \"clinicaltrial_2021\" in fileroot:\n",
    "    # Define column names for clinicaltrial_2020 and clinicaltrial_2021\n",
    "    # Assume the 'Id' column name is 'Id' and there is no 'Study Title' column\n",
    "    id_col = \"Id\"\n",
    "    title_col = None\n",
    "else:\n",
    "    # Default case (modify as needed based on data)\n",
    "    id_col = \"Id\"\n",
    "    title_col = \"Study Title\"\n",
    "\n",
    "# Call the function to calculate distinct counts\n",
    "if title_col is not None:\n",
    "    results = count_distinct_studies(clinicaltrial_df, id_col, title_col)\n",
    "else:\n",
    "    # If there's no 'Study Title' column, only calculate the distinct count for 'Id'\n",
    "    results = count_distinct_studies(clinicaltrial_df, id_col, id_col)\n",
    "\n",
    "# Display the dictionary of results (optional)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc65d5c-e9a2-4fde-a011-93f3df87089d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_first_20_distinct_ids(df, id_col):\n",
    "    \"\"\"\n",
    "    Display the first 20 distinct Ids from the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing clinical trial data.\n",
    "        id_col (str): The column name for 'Id'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get the distinct Ids\n",
    "    distinct_ids = df.select(id_col).distinct()\n",
    "\n",
    "    # Select the first 20 distinct Ids\n",
    "    first_20_distinct_ids = distinct_ids.limit(20)\n",
    "\n",
    "    # Display the first 20 distinct Ids\n",
    "    print(f\"First 20 distinct Ids in column '{id_col}':\")\n",
    "    display(first_20_distinct_ids)\n",
    "\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with\n",
    "# Define the column name for 'Id'\n",
    "id_col = \"Id\"  # Assuming the column name for 'Id' is 'Id'\n",
    "\n",
    "# Call the function to display the first 20 distinct Ids\n",
    "display_first_20_distinct_ids(clinicaltrial_df, id_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae3a508-dba4-488f-90ee-791c1189acc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def display_first_20_distinct_study_titles(df: DataFrame, title_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Display the first 20 distinct study titles from the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing clinical trial data.\n",
    "        title_col (str): The column name for 'Study Title'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if the title_col exists in the DataFrame\n",
    "    if title_col not in df.columns:\n",
    "        print(f\"The column '{title_col}' does not exist in the DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Get the distinct study titles\n",
    "    distinct_study_titles = df.select(title_col).distinct()\n",
    "\n",
    "    # Select the first 20 distinct study titles\n",
    "    first_20_distinct_study_titles = distinct_study_titles.limit(20)\n",
    "\n",
    "    # Display the first 20 distinct study titles\n",
    "    print(f\"First 20 distinct study titles in column '{title_col}':\")\n",
    "    display(first_20_distinct_study_titles)\n",
    "\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with\n",
    "# Define the column name for 'Study Title'\n",
    "title_col = \"Study Title\"  # Assuming the column name for 'Study Title' is 'Study Title'\n",
    "\n",
    "# Call the function to display the first 20 distinct study titles\n",
    "display_first_20_distinct_study_titles(clinicaltrial_df, title_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e774b5-1788-4251-a5c4-13d1e9aaaebf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_distinct_studies(df, id_col, title_col=None):\n",
    "    \"\"\"\n",
    "    Plot the distinct number of studies based on 'Id' and 'Study Title'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing clinical trial data.\n",
    "        id_col (str): The column name for 'Id'.\n",
    "        title_col (str, optional): The column name for 'Study Title'. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Count distinct studies based on 'Id'\n",
    "    distinct_studies_id = df.select(id_col).distinct().count()\n",
    "\n",
    "    # Initialize the count of distinct studies based on 'Study Title' to 0\n",
    "    distinct_studies_title = 0\n",
    "\n",
    "    # If 'Study Title' column exists in the DataFrame, count distinct studies based on 'Study Title'\n",
    "    if title_col and title_col in df.columns:\n",
    "        distinct_studies_title = df.select(title_col).distinct().count()\n",
    "\n",
    "    # Create a DataFrame for the data\n",
    "    data = {\n",
    "        'Category': ['Distinct Studies (Id)', 'Distinct Studies (Study Title)'],\n",
    "        'Count': [distinct_studies_id, distinct_studies_title]\n",
    "    }\n",
    "\n",
    "    df_counts = pd.DataFrame(data)\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(df_counts['Category'], df_counts['Count'], color=['blue', 'green'])\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distinct Number of Studies Based on \"Id\" and \"Study Title\"')\n",
    "\n",
    "    # Add hover-over data: annotate bars\n",
    "    for bar in bars:\n",
    "        # Get the height of the bar (count value)\n",
    "        yval = bar.get_height()\n",
    "        # Add text annotation above the bar to display the count value\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f'{int(yval)}',\n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with\n",
    "\n",
    "# Define the column names for 'Id' and 'Study Title'\n",
    "id_col = \"Id\"  # Column name for 'Id'\n",
    "title_col = \"Study Title\"  # Column name for 'Study Title'\n",
    "\n",
    "# Call the function to plot the distinct studies\n",
    "plot_distinct_studies(clinicaltrial_df, id_col, title_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa88020-5727-4034-85f7-a45f7b40140d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 2. You should list all the types (as contained in the Type column) of studies in the dataset along with the frequencies of each type. These should be ordered from most frequent to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05112601-ce5b-4872-94a3-3a6d2f641298",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def group_and_count(df: DataFrame, group_col: str, order_desc: bool = True):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by a specified column and count the occurrences.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing clinical trial data.\n",
    "        group_col (str): The column name to group by.\n",
    "        order_desc (bool): Whether to order the results in descending order based on the count. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Group by the specified column and count the occurrences of each group\n",
    "    grouped_frequencies = df.groupBy(group_col).count()\n",
    "\n",
    "    # If ordering in descending order is specified, order the result\n",
    "    if order_desc:\n",
    "        grouped_frequencies = grouped_frequencies.orderBy(desc(\"count\"))\n",
    "\n",
    "    # Display the result\n",
    "    display(grouped_frequencies)\n",
    "\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with\n",
    "# Define the column name to group by\n",
    "group_col = \"Type\"  # Example: you can change this to the column you want to group by\n",
    "\n",
    "# Call the function to group and count the DataFrame\n",
    "group_and_count(clinicaltrial_df, group_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92bb254e-1fa2-4d3e-bc84-c91b136a7405",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "def plot_type_frequencies(df: DataFrame, group_col: str):\n",
    "    \"\"\"\n",
    "    Plot the frequencies of each type of study in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing clinical trial data.\n",
    "        group_col (str): The column name to group by.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Group by the specified column and count the occurrences of each group\n",
    "    type_frequencies = df.groupBy(group_col).count()\n",
    "\n",
    "    # Order the result in descending order based on the count\n",
    "    type_frequencies_ordered = type_frequencies.orderBy(desc(\"count\"))\n",
    "\n",
    "    # Convert the Spark DataFrame to Pandas DataFrame\n",
    "    type_frequencies_df = type_frequencies_ordered.toPandas()\n",
    "\n",
    "    # Check for null values and replace them with 'Unknown'\n",
    "    type_frequencies_df[group_col] = type_frequencies_df[group_col].fillna(\"Unknown\")\n",
    "\n",
    "    # Plotting the bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(type_frequencies_df[group_col], type_frequencies_df[\"count\"], color='blue')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(f\"{group_col} of Study\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Frequencies of Each {group_col} of Study in the Dataset\")\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Display the chart\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with\n",
    "# Define the column name to group by\n",
    "group_col = \"Type\"  # Example: you can change this to the column you want to group by\n",
    "\n",
    "# Call the function to plot the frequencies\n",
    "plot_type_frequencies(clinicaltrial_df, group_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cdc4c7-bbdc-4b06-9a0e-6f350b46c12b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 3. The top 5 conditions (from Conditions) with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62dc5a7c-1438-426b-8171-b349b89d2a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import desc, col, split, explode, trim, isnull\n",
    "\n",
    "def display_top_5_conditions(clinicaltrial_df: DataFrame, fileroot: str):\n",
    "    \"\"\"\n",
    "    Display the top 5 conditions and their frequencies based on the given fileroot.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): The DataFrame containing the data to work with.\n",
    "        fileroot (str): The root of the file to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # For clinicaltrial_2023, group by \"Conditions\" column and count occurrences\n",
    "        condition_frequencies = clinicaltrial_df.groupBy(\"Conditions\").count()\n",
    "        # Order the result from most frequent to least frequent and limit to the top 5\n",
    "        top_5_conditions = condition_frequencies.orderBy(desc(\"count\")).limit(5)\n",
    "        # Display the top 5 conditions with their frequencies\n",
    "        top_5_conditions.show()\n",
    "\n",
    "    elif fileroot in (\"clinicaltrial_2020\", \"clinicaltrial_2021\"):\n",
    "        # For clinicaltrial_2020 and clinicaltrial_2021, split the 'conditions' column by commas\n",
    "        conditions_df = clinicaltrial_df.select(split('conditions', ',').alias('condition_list'))\n",
    "        # Use explode to create a new DataFrame with one row per condition\n",
    "        exploded_conditions_df = conditions_df.select(explode('condition_list').alias('condition'))\n",
    "        # Filter out empty strings and whitespace-only conditions\n",
    "        exploded_conditions_df = exploded_conditions_df.filter(\n",
    "            ~isnull(exploded_conditions_df['condition']) & (trim(exploded_conditions_df['condition']) != \"\")\n",
    "        )\n",
    "        # Calculate the frequency of each condition using groupBy and count\n",
    "        condition_frequency_df = exploded_conditions_df.groupBy('condition').count()\n",
    "        # Order the conditions by frequency in descending order and limit to the top 5\n",
    "        top_5_conditions_df = condition_frequency_df.orderBy(desc('count')).limit(5)\n",
    "        # Display the top 5 conditions and their frequencies\n",
    "        print(\"Top 5 conditions and their frequencies:\")\n",
    "        top_5_conditions_df.show()\n",
    "\n",
    "display_top_5_conditions(clinicaltrial_df, fileroot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd5e501-b347-4a11-bbce-1d8b6d03720d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install mplcursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a98c374-4a1d-4d6f-adc9-a41c3edb08d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplcursors  # Importing mplcursors for hover functionality\n",
    "\n",
    "def plot_top_5_conditions(clinicaltrial_df, fileroot):\n",
    "    \"\"\"\n",
    "    Plot a bar chart for the top 5 conditions with their frequencies in a given DataFrame\n",
    "    and incorporate hover functionality using mplcursors to display tooltips.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): The DataFrame containing the data to work with.\n",
    "        fileroot (str): The root of the file to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Group by the \"Conditions\" column and count the occurrences of each condition\n",
    "        condition_frequencies = clinicaltrial_df.groupBy(\"Conditions\").count()\n",
    "        # Order the result from most frequent to least frequent and limit to the top 5\n",
    "        top_5_conditions = condition_frequencies.orderBy(desc(\"count\")).limit(5)\n",
    "        # Convert the Spark DataFrame to a Pandas DataFrame\n",
    "        top_5_conditions_df = top_5_conditions.toPandas()\n",
    "\n",
    "    elif fileroot in (\"clinicaltrial_2020\", \"clinicaltrial_2021\"):\n",
    "        # Step 1: Split the 'conditions' column by commas and alias it as 'condition_list'\n",
    "        conditions_df = clinicaltrial_df.select(split('conditions', ',').alias('condition_list'))\n",
    "        # Step 2: Use explode to create a new DataFrame with one row per condition\n",
    "        exploded_conditions_df = conditions_df.select(explode('condition_list').alias('condition'))\n",
    "        # Step 3: Filter out empty strings and whitespace-only conditions\n",
    "        exploded_conditions_df = exploded_conditions_df.filter(\n",
    "            ~isnull(exploded_conditions_df['condition']) & (trim(exploded_conditions_df['condition']) != \"\")\n",
    "        )\n",
    "        # Step 4: Calculate the frequency of each condition using groupBy and count\n",
    "        condition_frequency_df = exploded_conditions_df.groupBy('condition').count()\n",
    "        # Order the conditions by frequency in descending order and limit to the top 5\n",
    "        top_5_conditions_df = condition_frequency_df.orderBy(desc('count')).limit(5)\n",
    "        # Convert the result to a Pandas DataFrame\n",
    "        top_5_conditions_df = top_5_conditions_df.toPandas()\n",
    "\n",
    "    # Plotting the top 5 conditions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bars = plt.bar(\n",
    "        top_5_conditions_df['condition' if fileroot in (\"clinicaltrial_2020\", \"clinicaltrial_2021\") else \"Conditions\"],\n",
    "        top_5_conditions_df['count'],\n",
    "        color='blue'\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Condition')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 5 Conditions with Frequencies')\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Enable hover functionality using mplcursors\n",
    "    cursor = mplcursors.cursor(bars, hover=True)\n",
    "\n",
    "    # Define the tooltip content\n",
    "    def on_add(sel):\n",
    "        condition = top_5_conditions_df.iloc[sel.index][0]  # Access condition value based on fileroot\n",
    "        count = top_5_conditions_df.iloc[sel.index][1]\n",
    "        sel.annotation.set_text(f'Condition: {condition}\\nCount: {count}')\n",
    "\n",
    "    # Connect the cursor to the on_add function\n",
    "    cursor.connect(\"add\", on_add)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'clinicaltrial_df' is the DataFrame you want to work with and 'fileroot' is the identifier\n",
    "plot_top_5_conditions(clinicaltrial_df, fileroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07e97ba-0860-4cc4-9ef2-a4ad9ebdcbfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, regexp_replace, split, explode, trim, expr, lower, count\n",
    "\n",
    "def process_conditions(clinicaltrial_df: DataFrame, fileroot: str):\n",
    "    \"\"\"\n",
    "    Process the conditions based on the fileroot value.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): The DataFrame containing the data to work with.\n",
    "        fileroot (str): The root of the file to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Step 1: Split the 'Conditions' column by pipe ('|') to create an array\n",
    "        cleaned_df = clinicaltrial_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            split(col(\"Conditions\"), r\"\\|\")\n",
    "        )\n",
    "\n",
    "        # Step 2: Explode the 'Conditions' array to separate rows for each condition\n",
    "        exploded_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            explode(col(\"Conditions\"))\n",
    "        )\n",
    "\n",
    "        # Step 3: Group the exploded data by 'Conditions' and count occurrences\n",
    "        condition_counts = exploded_df.groupBy(\"Conditions\").count()\n",
    "\n",
    "        # Step 4: Order the DataFrame by count in descending order\n",
    "        condition_counts_sorted = condition_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "        # Step 5: Select the top 5 conditions\n",
    "        top_5_conditions = condition_counts_sorted.limit(5)\n",
    "\n",
    "        # Display the top 5 conditions with their frequencies\n",
    "        top_5_conditions.show()\n",
    "\n",
    "        # Convert PySpark DataFrame to Pandas DataFrame\n",
    "        top_5_conditions_df = top_5_conditions.toPandas()\n",
    "\n",
    "        # Plot the top 5 conditions with their frequencies\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='count', y='Conditions', data=top_5_conditions_df)\n",
    "        plt.title('Top 5 Conditions with Their Frequencies')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Conditions')\n",
    "        plt.show()\n",
    "\n",
    "    elif fileroot in (\"clinicaltrial_2020\", \"clinicaltrial_2021\"):\n",
    "        print(f\"This operation is not applicable to {fileroot} file.\")\n",
    "\n",
    "# Call the function with the appropriate DataFrame and fileroot\n",
    "process_conditions(clinicaltrial_df, fileroot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b59404-2086-430d-98cf-1bc3230353b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Additional Cleaning for Question 3 The top 5 conditions (from Conditions) with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d589c0ea-503f-4503-8b97-a7aa17431f4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, regexp_replace, split, explode, trim, expr, lower\n",
    "\n",
    "def process_clinicaltrial_data(clinicaltrial_df: DataFrame, fileroot: str):\n",
    "    \"\"\"\n",
    "    Process the clinical trial data based on the fileroot.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): The DataFrame containing the data to work with.\n",
    "        fileroot (str): The root of the file to determine which operations to perform.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Step 1: Remove quotes and clean the 'Conditions' column and trim leading and trailing spaces\n",
    "        cleaned_df = clinicaltrial_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            trim(regexp_replace(col(\"Conditions\").cast(\"string\"), r'\"', ''))\n",
    "        )\n",
    "        # Step 2: Replace square brackets (`[` and `]`) and parentheses (`(` and `)`) with no space in 'Conditions' column\n",
    "        cleaned_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            regexp_replace(col(\"Conditions\"), r'[\\[\\]\\(\\)]', '')\n",
    "        )\n",
    "        # Step 3: Split the 'Conditions' column by pipe ('|') to create an array\n",
    "        cleaned_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            split(col(\"Conditions\"), r\"\\|\")\n",
    "        )\n",
    "        # Step 4: Explode the 'Conditions' array to separate rows for each condition\n",
    "        cleaned_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            explode(col(\"Conditions\"))\n",
    "        )\n",
    "        # Step 5: Split each condition by comma (',') and flatten the nested conditions\n",
    "        cleaned_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            split(col(\"Conditions\"), \",\")\n",
    "        )\n",
    "        # Step 6: Explode the array created in Step 5\n",
    "        cleaned_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            explode(col(\"Conditions\"))\n",
    "        )\n",
    "        # Step 7: Trim leading and trailing whitespaces from the 'Conditions' column\n",
    "        cleaned_df = cleaned_df.withColumn(\n",
    "            \"Conditions\",\n",
    "            trim(col(\"Conditions\"))\n",
    "        )\n",
    "        # Step 8: Filter out rows with the condition 'e.g.' in a case-insensitive manner\n",
    "        filtered_df = cleaned_df.filter(\n",
    "            ~lower(col(\"Conditions\")).like(\"e.g.\")\n",
    "        )\n",
    "\n",
    "        # Calculate the top 5 conditions with their frequencies\n",
    "        top_5_conditions = filtered_df.groupBy(\"Conditions\").count()  # Group by 'Conditions' and count occurrences\n",
    "        top_5_conditions = top_5_conditions.orderBy(col(\"count\").desc())  # Order by count in descending order\n",
    "        top_5_conditions = top_5_conditions.limit(5)  # Select the top 5 conditions\n",
    "\n",
    "        # Display the top 5 conditions with their frequencies\n",
    "        top_5_conditions.show()\n",
    "\n",
    "        # Convert the top 5 conditions to a Pandas DataFrame\n",
    "        top_5_conditions_df = top_5_conditions.toPandas()\n",
    "\n",
    "        # Plot the top 5 conditions with their frequencies\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='count', y='Conditions', data=top_5_conditions_df)\n",
    "        plt.title('Top 5 Conditions with Their Frequencies')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Conditions')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"This operation is not applicable to {fileroot} file.\")\n",
    "\n",
    "# Call the function with the appropriate DataFrame and fileroot\n",
    "process_clinicaltrial_data(clinicaltrial_df, fileroot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb4d68c-b7b4-4f4a-ac58-2b15fbffd3f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(clinicaltrial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649a4d0d-dbe6-4d98-9877-b4756a4ca090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fileroot1= \"pharma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c90de7-9c29-4275-b5b7-e421109c2f91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Copying the file to /tmp directory on driver node\n",
    "dbutils.fs.cp(\"/FileStore/tables/\" + fileroot1+\".zip\", \"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692ed08f-43f2-487b-b3be-d7347d733b03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139e0ae3-1038-4cab-8154-7a39fc132d87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the fileroot.zip in local tmp directory\n",
    "dbutils.fs.ls(\"file:/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486547be-30c6-48c8-abc4-f96c5224dba5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Making 'fileroot' accessible by the command line\n",
    "import os\n",
    "os.environ['fileroot1'] = fileroot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3ee0e1-c225-44a7-8954-f2764a4a064f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Unzipping the file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c838a5b4-cc6f-4d2c-8009-37de75e48925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "unzip -d /tmp /tmp/$fileroot1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22731fe7-1b5c-46a2-897c-d866044de4c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Moving the unzipped file to the DBFS directory\n",
    "dbutils.fs.mv(\"file:/tmp/\" + fileroot1 +\".csv\" , \"/FileStore/tables/\", True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549285a5-488c-489a-bfbc-7f7526aaab3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the unzipped file in DBFS directory\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84abf707-42fe-43e7-a896-301168852743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking the DBFS directory\n",
    "dbutils.fs.ls (\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae222cb-de99-49a6-a441-24aefe70ba07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Listing the contents of the csv file\n",
    "dbutils.fs.head(\"/FileStore/tables/\" + fileroot1 + \".csv\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36993ff-8d5e-4f3f-9fc3-485be9975c64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 4. Find the 10 most common sponsors that are not pharmaceutical companies, along with the number of clinical trials they have sponsored. Hint: For a basic implementation, you can assume that the Parent Company column contains all possible pharmaceutical companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb88a1b7-437c-4168-89dc-86aae63a936f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading the csv data into spark data frame using read.csv method\n",
    "pharmaDF = spark.read.options(delimiter = ',').csv(\"/FileStore/tables/\"+ fileroot1 + \".csv\",\n",
    "                             header = \"true\",\n",
    "                             inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cb9e9f-89f9-478a-aad0-901a8c8135b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pharmaDF.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a84f5f0-fd86-4124-971b-05534d3bdd5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#printing the schema for pharma datset\n",
    "pharmaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a4ed88-45e6-4a73-ac47-bb7c7b187f81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, desc\n",
    " # From parent company column, extracting the list of all pharmaceutical companies\n",
    "pharmaceutical_companies = [row[0] for row in pharmaDF.select('Parent_Company').distinct().collect()]\n",
    " \n",
    "# Using clinical trial Sponsor column, filtering out all non pharmaceutical companies\n",
    "non_pharmaceutical_sponsors = clinicaltrial_df.filter(~clinicaltrial_df['Sponsor'].isin(pharmaceutical_companies))\n",
    " \n",
    "# Count the number of clinical trials sponsored by each sponsor and sort in descending order\n",
    "tenTopSponsors = non_pharmaceutical_sponsors.groupBy('Sponsor') \\\n",
    "                                          .agg(count(\"*\").alias(\"numberOfTrials\")) \\\n",
    "                                          .sort(desc(\"numberOfTrials\"))\n",
    "                                          \n",
    "# Show the top 10 sponsors\n",
    "tenTopSponsors.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ef9332-6d47-4c65-9fbe-94418b52dbf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Convert the top 10 sponsors DataFrame to Pandas DataFrame\n",
    "top_10_sponsors = tenTopSponsors.limit(10).toPandas()\n",
    "\n",
    "# Create a colormap\n",
    "cmap = cm.get_cmap('viridis', 10)  # Choose a colormap, e.g., 'viridis'\n",
    "\n",
    "# Normalize the `numberOfTrials` values to use with the colormap\n",
    "norm = plt.Normalize(top_10_sponsors['numberOfTrials'].min(), top_10_sponsors['numberOfTrials'].max())\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))  # Adjusting figure size\n",
    "plt.barh(\n",
    "    top_10_sponsors['Sponsor'], \n",
    "    top_10_sponsors['numberOfTrials'], \n",
    "    color=cmap(norm(top_10_sponsors['numberOfTrials']))\n",
    ")\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Sponsor')\n",
    "plt.title('Top 10 Non-Pharmaceutical Sponsors by Number of Clinical Trials')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis for better visualization\n",
    "plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), label='Number of Trials')  # Add a color bar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183abc13-d3de-41fd-ad4c-515abfe172d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 5. Plot number of completed studies for each month in 2023. You need to include your visualization as well as a table of all the values you have plotted for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5223c085-94c7-4524-b584-abf0f7c56422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def process_clinicaltrial(clinicaltrial_df: DataFrame, fileroot: str, trial_year: int):\n",
    "    \"\"\"\n",
    "    Process clinicaltrial data and generate bar and scatter plots for completed studies each month in the specified trial year.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): Spark DataFrame containing the clinicaltrial data.\n",
    "        fileroot (str): The file root string to determine which operations to perform.\n",
    "        trial_year (int): The year of interest for the analysis.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == \"clinicaltrial_2023\":\n",
    "        # Convert the 'Start' and 'Completion' columns to timestamp data type\n",
    "        clinicaltrial_df = clinicaltrial_df.withColumn(\"Start\", col(\"Start\").cast(\"timestamp\"))\n",
    "        clinicaltrial_df = clinicaltrial_df.withColumn(\"Completion\", col(\"Completion\").cast(\"timestamp\"))\n",
    "\n",
    "        # Function to plot studies each month for the specified completion status and year\n",
    "        def plot_studies_each_month(completion_status, study_year):\n",
    "            # Convert the completion status parameter to lowercase\n",
    "            completion_status = completion_status.lower()\n",
    "            \n",
    "            # Filter the DataFrame for studies with the specified completion status and in the specified study year\n",
    "            studies_completed = clinicaltrial_df.filter(\n",
    "                (F.lower(clinicaltrial_df.Status) == completion_status) & \n",
    "                (F.year(clinicaltrial_df.Completion) == study_year)\n",
    "            )\n",
    "\n",
    "            # Extract the month from the 'Completion' column\n",
    "            studies_completed = studies_completed.withColumn(\n",
    "                \"month\", F.month(clinicaltrial_df.Completion)\n",
    "            )\n",
    "            \n",
    "            # Group by the extracted month and count the number of studies\n",
    "            monthly_basis_results = studies_completed.groupBy(\"month\").count()\n",
    "            \n",
    "            # Display the count of completed studies for each month, ordered by month\n",
    "            monthly_basis_results.orderBy(\"month\").show()\n",
    "            \n",
    "            # Collect the results to lists for plotting\n",
    "            month_number = [row.month for row in monthly_basis_results.collect()]\n",
    "            no_of_completed_studies = [row[\"count\"] for row in monthly_basis_results.collect()]\n",
    "            \n",
    "            return month_number, no_of_completed_studies\n",
    "\n",
    "        # Call the function with the desired completion status and study year\n",
    "        month_number, no_of_completed_studies = plot_studies_each_month(\"Completed\", trial_year)\n",
    "\n",
    "        # Create a bar graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(month_number, no_of_completed_studies, color='blue')\n",
    "\n",
    "        # Add counts on top of each bar\n",
    "        for bar in bars:\n",
    "            y_val = bar.get_height()  # Get the height of the bar (count)\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2,  # Center the text over the bar\n",
    "                y_val,  # Place the text at the top of the bar\n",
    "                str(y_val),  # Convert count to string\n",
    "                ha='center',  # Center-align the text\n",
    "                va='bottom'   # Place the text at the bottom (above the bar)\n",
    "            )\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Number of Completed Studies')\n",
    "        plt.title(f'Bar Graph of Completed Studies Each Month in {trial_year}')\n",
    "\n",
    "        # Display the bar plot\n",
    "        plt.show()\n",
    "\n",
    "        # Create a scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(month_number, no_of_completed_studies, color='blue', marker='o')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Number of Completed Studies')\n",
    "        plt.title(f'Scatter Plot of Completed Studies Each Month in {trial_year}')\n",
    "\n",
    "        # Display the scatter plot\n",
    "        plt.show()\n",
    "    \n",
    "    elif fileroot in [\"clinicaltrial_2020\", \"clinicaltrial_2021\"]:\n",
    "        # Filter the DataFrame to only include completed studies in the specified trial year\n",
    "        completed_studies_df = clinicaltrial_df.filter(\n",
    "            (F.col('Status') == 'Completed') &\n",
    "            (F.year(F.to_date(F.col('Completion'), 'MMM yyyy')) == trial_year)\n",
    "        )\n",
    "\n",
    "        # Group the data by month and count the number of completed studies\n",
    "        monthly_completed_studies_df = completed_studies_df.withColumn(\n",
    "            'month_year',\n",
    "            F.date_format(F.to_date(F.col('Completion'), 'MMM yyyy'), 'yyyy-MM')\n",
    "        ).groupBy('month_year').count()\n",
    "\n",
    "        # Order the DataFrame by 'month_year' in ascending order\n",
    "        monthly_completed_studies_df = monthly_completed_studies_df.orderBy('month_year', ascending=True)\n",
    "\n",
    "        # Show the monthly completed studies in ascending order of months\n",
    "        monthly_completed_studies_df.show()\n",
    "\n",
    "        # Convert the DataFrame to a Pandas DataFrame\n",
    "        monthly_completed_studies_pd_df = monthly_completed_studies_df.toPandas()\n",
    "\n",
    "        # Plotting the data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(monthly_completed_studies_pd_df['month_year'], monthly_completed_studies_pd_df['count'], marker='o')\n",
    "        plt.title(f'Number of Completed Studies Each Month in {trial_year}')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Number of Completed Studies')\n",
    "        plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability\n",
    "        plt.grid(True)  # Add grid lines\n",
    "        plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "process_clinicaltrial(clinicaltrial_df, fileroot, trial_Year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bceeb4a2-b3c6-4acb-8084-0f80cadc02e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Additional Analyses and its Visualization only for Clinicaltrial_2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a18df0-e509-4225-86d4-71b5c65b7082",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1. Analysis of Study Status Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22ab8b9-e71e-4eb6-9463-26fff18c4cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_clinicaltrial_status(clinicaltrial_df, fileroot):\n",
    "    \"\"\"\n",
    "    Analyze the status distribution in a clinical trial DataFrame and plot the distribution.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): A Spark DataFrame containing the clinical trial data.\n",
    "        fileroot (str): The root identifier of the file, e.g., 'clinicaltrial_2023', 'clinicaltrial_2020', etc.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == 'clinicaltrial_2023':\n",
    "        # Filter the DataFrame to include only records with 'completed' or 'recruiting' status\n",
    "        status_filtered_df = clinicaltrial_df.filter(\n",
    "            (func.lower(clinicaltrial_df.Status) == 'completed') |\n",
    "            (func.lower(clinicaltrial_df.Status) == 'recruiting')\n",
    "        )\n",
    "\n",
    "        # Group by the 'Status' column and count the frequency of each status\n",
    "        status_distribution = status_filtered_df.groupBy(\"Status\").count()\n",
    "\n",
    "        # Display the status distribution\n",
    "        status_distribution.show()\n",
    "\n",
    "        # Plot the status distribution\n",
    "        status_labels = [row.Status for row in status_distribution.collect()]\n",
    "        status_counts = [row[\"count\"] for row in status_distribution.collect()]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(status_labels, status_counts, color='blue')\n",
    "        plt.xlabel('Status')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Study Statuses: Completed and Recruiting')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"This additional analysis is not applicable for fileroot = {fileroot}\")\n",
    "\n",
    "\n",
    "analyze_clinicaltrial_status(clinicaltrial_df, fileroot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a1a777-9ecf-4fcb-b905-370f09eb996d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2. Analysis of Start Dates of Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e0f52a-73e7-4e77-84b9-1204b4860403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_study_start_dates(clinicaltrial_df, fileroot):\n",
    "    \"\"\"\n",
    "    Analyze and plot the distribution of study start dates in a clinical trial DataFrame.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): A Spark DataFrame containing the clinical trial data.\n",
    "        fileroot (str): The root identifier of the file, e.g., 'clinicaltrial_2023', 'clinicaltrial_2020', etc.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == 'clinicaltrial_2023':\n",
    "        # Group the data by year and month of the 'Start' column\n",
    "        start_dates_distribution = clinicaltrial_df.withColumn(\"year\", func.year(\"Start\")) \\\n",
    "            .withColumn(\"month\", func.month(\"Start\")) \\\n",
    "            .groupBy(\"year\", \"month\") \\\n",
    "            .count()\n",
    "\n",
    "        # Display the results\n",
    "        start_dates_distribution.show()\n",
    "\n",
    "        # Filter the data to include only the years from 2015 to 2023\n",
    "        filtered_data = start_dates_distribution.filter(\n",
    "            (start_dates_distribution.year >= 2015) & (start_dates_distribution.year <= 2023)\n",
    "        )\n",
    "\n",
    "        # Create a scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Create a color palette with 12 colors (one for each month)\n",
    "        colors = sns.color_palette(\"husl\", 12)\n",
    "\n",
    "        # Iterate through each month and plot a scatter plot with each month in different colors\n",
    "        for month in range(1, 13):  # Months from 1 to 12\n",
    "            # Filter the data for the current month\n",
    "            monthly_data = filtered_data.filter(filtered_data.month == month)\n",
    "\n",
    "            # Extract year and count for the current month\n",
    "            years = [row.year for row in monthly_data.collect()]\n",
    "            counts = [row[\"count\"] for row in monthly_data.collect()]\n",
    "\n",
    "            # Plot the data points for the current month in a specific color\n",
    "            plt.scatter(years, counts, color=colors[month - 1], label=f'Month {month}', alpha=0.7)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Studies Started')\n",
    "        plt.title('Distribution of Study Start Dates from 2015 to 2023')\n",
    "\n",
    "        # Add a legend with a reduced font size\n",
    "        plt.legend(loc='upper left', fontsize='small')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"This additional analysis is not applicable for fileroot = {fileroot}\")\n",
    "\n",
    "\n",
    "analyze_study_start_dates(clinicaltrial_df, fileroot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ea3bf7-930c-4776-8286-852789d08ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3. Analysis on Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bce59f-c758-4594-9385-1bd84b681bd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9473d7ec-5837-4729-95e5-38b6e2fe3138",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_interventions(clinicaltrial_df, fileroot):\n",
    "    \"\"\"\n",
    "    Analyze the top 5 intervention types in a clinical trial DataFrame.\n",
    "\n",
    "    Args:\n",
    "        clinicaltrial_df (DataFrame): A Spark DataFrame containing the clinical trial data.\n",
    "        fileroot (str): The root identifier of the file, e.g., 'clinicaltrial_2023', 'clinicaltrial_2020', etc.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if fileroot == 'clinicaltrial_2023':\n",
    "        # Step 1: Clean the 'Interventions' column by replacing all double quotes (\") with no space\n",
    "        cleaned_interventions_df = clinicaltrial_df.withColumn(\n",
    "            \"Cleaned_Interventions\",\n",
    "            func.regexp_replace(clinicaltrial_df[\"Interventions\"], '\"', '')\n",
    "        )\n",
    "\n",
    "        # Step 2: Extract the word before the colon (:)\n",
    "        cleaned_interventions_df = cleaned_interventions_df.withColumn(\n",
    "            \"Intervention_Type\",\n",
    "            func.split(cleaned_interventions_df[\"Cleaned_Interventions\"], \":\").getItem(0)\n",
    "        )\n",
    "\n",
    "        # Step 3: Remove leading and trailing spaces from the intervention type\n",
    "        cleaned_interventions_df = cleaned_interventions_df.withColumn(\n",
    "            \"Intervention_Type\",\n",
    "            func.trim(cleaned_interventions_df[\"Intervention_Type\"])\n",
    "        )\n",
    "\n",
    "        # Step 4: Group by 'Intervention_Type' and count the occurrences, order by count in descending order\n",
    "        intervention_counts = cleaned_interventions_df.groupBy(\"Intervention_Type\").count().orderBy(func.desc(\"count\"))\n",
    "\n",
    "        # Step 5: Display only the top 5 counts\n",
    "        top_5_intervention_counts = intervention_counts.limit(5).collect()\n",
    "        for row in top_5_intervention_counts:\n",
    "            print(row)\n",
    "\n",
    "        # Convert the top 5 intervention counts to a pandas DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Intervention_Type': [row['Intervention_Type'] for row in top_5_intervention_counts],\n",
    "            'Count': [row['count'] for row in top_5_intervention_counts]\n",
    "        })\n",
    "\n",
    "        # Create an interactive bar chart using Plotly\n",
    "        fig = px.bar(\n",
    "            df,\n",
    "            x='Intervention_Type',\n",
    "            y='Count',\n",
    "            color='Count',\n",
    "            color_continuous_scale='Reds',\n",
    "            title='Top 5 Intervention Types and Their Counts'\n",
    "        )\n",
    "\n",
    "        # Customize the layout for better visibility\n",
    "        fig.update_layout(\n",
    "            xaxis_title='Intervention Types',\n",
    "            yaxis_title='Count'\n",
    "        )\n",
    "\n",
    "        # Display the interactive plot\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(f\"This additional analysis is not applicable for fileroot = {fileroot}\")\n",
    "\n",
    "\n",
    "analyze_interventions(clinicaltrial_df, fileroot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022326db-b4cb-4a6a-baac-e343bfba46a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Visualization in POWERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07cd230-effd-4fd0-b0a6-f02a68bd3fb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(clinicaltrial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9070dcde-54c3-48f3-b13e-5976741c830c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Changing the Start and Completion columns to timestamp data type\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "clinicaltrial_df = clinicaltrial_df.withColumn(\"Start\",col(\"Start\").cast(\"timestamp\"))\n",
    "clinicaltrial_df = clinicaltrial_df.withColumn(\"Completion\",col(\"Completion\").cast(\"timestamp\"))\n",
    "display(clinicaltrial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a5a93f8-e821-4088-aca3-5a27b7114aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to a CSV file, overwriting any existing file at the specified path\n",
    "clinicaltrial_df.write.mode(\"overwrite\").csv(\"/FileStore/tables/CT2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df62299-9b4c-4ec7-a3fe-793c9b2c747b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists CT2023;\n",
    "create table CT2023\n",
    "using csv\n",
    "options (path \"dbfs:/FileStore/tables/CT2023.csv\", header \"False\" , inferSchema \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad451603-d96a-47a0-a0b8-94b5a331bee6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3c7d11-f4ea-44b1-9f5e-1fc378f6385f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM CT2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292e2a74-6616-4c5b-bb63-5d6d948843a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path of the directory you want to delete\n",
    "directory_path = \"/FileStore/tables/\"\n",
    "\n",
    "# Use dbutils.fs.rm() to delete all files and directories in the specified location\n",
    "dbutils.fs.rm(directory_path, recurse=True)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"All files and directories in the location {directory_path} have been deleted.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3947831324461349,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Remya_Vellakkadaparambu_Karthikeyan_df",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
